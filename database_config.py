"""
ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®šã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ 
PostgreSQLãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ¥ç¶šã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ã‚’æä¾›
"""

import os
import json
import threading
import psycopg2
from psycopg2.extras import RealDictCursor
from datetime import datetime, timedelta
from dotenv import load_dotenv
from utils.logger_config import get_logger

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒ­ã‚¬ãƒ¼ã®åˆæœŸåŒ–
logger = get_logger(__name__)

# ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆå…¨ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã§å…±æœ‰ï¼‰
_shared_cache_instance = None

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ç”¨ã®ãƒ­ãƒƒã‚¯ï¼ˆè¤‡æ•°ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‹ã‚‰ã®åŒæ™‚ã‚¢ã‚¯ã‚»ã‚¹ã‚’é˜²ãï¼‰
_connection_lock = threading.Lock()

class TrendsCache:
    """ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        global _shared_cache_instance
        
        # ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼šæ—¢å­˜ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒã‚ã‚Œã°å†åˆ©ç”¨
        if _shared_cache_instance is not None:
            # æ—¢å­˜ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®å±æ€§ã‚’ã‚³ãƒ”ãƒ¼ï¼ˆæ¥ç¶šã‚’å…±æœ‰ï¼‰
            self.connection = _shared_cache_instance.connection
            return
        
        # åˆå›ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
        self.connection = None
        # æ¥ç¶šã‚’é…å»¶åˆæœŸåŒ–ï¼ˆã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ã‚¢ãƒ—ãƒªã‚’èµ·å‹•ã§ãã‚‹ã‚ˆã†ã«ï¼‰
        try:
            self.connect()
        except Exception as e:
            logger.warning(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆå¾Œã§å†è©¦è¡Œå¯èƒ½ï¼‰: {e}", exc_info=True)
            self.connection = None
        
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«ä¿å­˜
        _shared_cache_instance = self
    
    def connect(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ¥ç¶š"""
        # æ—¢å­˜ã®æ¥ç¶šã‚’é–‰ã˜ã‚‹ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
        if self.connection:
            try:
                if not self.connection.closed:
                    self.connection.close()
            except Exception:
                pass  # æ—¢ã«é–‰ã˜ã‚‰ã‚Œã¦ã„ã‚‹å ´åˆã¯ç„¡è¦–
            self.connection = None
        
        try:
            # DATABASE_URLãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯å„ªå…ˆçš„ã«ä½¿ç”¨ï¼ˆfly.ioãªã©ï¼‰
            database_url = os.getenv('DATABASE_URL')
            if database_url:
                # DATABASE_URLã®å½¢å¼: postgresql://user:password@host:port/database
                # æ¥ç¶šã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¨ã‚­ãƒ¼ãƒ—ã‚¢ãƒ©ã‚¤ãƒ–è¨­å®šã‚’è¿½åŠ 
                # Fly.ioã®PostgreSQLã¯æ¥ç¶šã‚’é–‰ã˜ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€ã‚ˆã‚Šç©æ¥µçš„ãªã‚­ãƒ¼ãƒ—ã‚¢ãƒ©ã‚¤ãƒ–è¨­å®šã‚’ä½¿ç”¨
                self.connection = psycopg2.connect(
                    database_url,
                    connect_timeout=10,  # 10ç§’ã«å»¶é•·ï¼ˆä¸¦åˆ—ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚ã®æ¥ç¶šç¢ºç«‹æ™‚é–“ã‚’è€ƒæ…®ï¼‰
                    keepalives=1,
                    keepalives_idle=10,  # 10ç§’ã§ã‚­ãƒ¼ãƒ—ã‚¢ãƒ©ã‚¤ãƒ–ã‚’é–‹å§‹
                    keepalives_interval=5,  # 5ç§’é–“éš”ã§ã‚­ãƒ¼ãƒ—ã‚¢ãƒ©ã‚¤ãƒ–ã‚’é€ä¿¡
                    keepalives_count=5  # 5å›å¤±æ•—ã¾ã§è¨±å®¹
                )
                # è‡ªå‹•ã‚³ãƒŸãƒƒãƒˆã‚’ç„¡åŠ¹åŒ–ï¼ˆãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³åˆ¶å¾¡ã®ãŸã‚ï¼‰
                self.connection.autocommit = False
                logger.info("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šæˆåŠŸ (DATABASE_URLä½¿ç”¨)")
            else:
                # å€‹åˆ¥ã®ç’°å¢ƒå¤‰æ•°ã‚’ä½¿ç”¨ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç’°å¢ƒãªã©ï¼‰
                self.connection = psycopg2.connect(
                    host=os.getenv('DB_HOST', 'localhost'),
                    port=os.getenv('DB_PORT', '5432'),
                    database=os.getenv('DB_NAME', 'trends_db'),
                    user=os.getenv('DB_USER', 'postgres'),
                    password=os.getenv('DB_PASSWORD', 'password'),
                    connect_timeout=10,
                    keepalives=1,
                    keepalives_idle=30,
                    keepalives_interval=10,
                    keepalives_count=3
                )
                # è‡ªå‹•ã‚³ãƒŸãƒƒãƒˆã‚’ç„¡åŠ¹åŒ–ï¼ˆãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³åˆ¶å¾¡ã®ãŸã‚ï¼‰
                self.connection.autocommit = False
                logger.info("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šæˆåŠŸ (å€‹åˆ¥ç’°å¢ƒå¤‰æ•°ä½¿ç”¨)")
        except Exception as e:
            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            self.connection = None
    
    def init_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’åˆæœŸåŒ–"""
        try:
            conn = self.get_connection()
            if not conn:
                return False
        except Exception as e:
            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return False
        
        try:
            with conn.cursor() as cursor:
                # ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆã®SQL
                create_tables_sql = """
                CREATE TABLE IF NOT EXISTS google_trends_cache (
                    id SERIAL PRIMARY KEY,
                    keyword VARCHAR(255) NOT NULL,
                    score INTEGER NOT NULL,
                    region VARCHAR(10) NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                );
                
                CREATE TABLE IF NOT EXISTS youtube_trends_cache (
                    id SERIAL PRIMARY KEY,
                    region_code VARCHAR(10),
                    trend_type VARCHAR(50) DEFAULT 'trending',
                    video_id VARCHAR(255),
                    title TEXT NOT NULL,
                    channel_title VARCHAR(255),
                    view_count INTEGER DEFAULT 0,
                    like_count INTEGER DEFAULT 0,
                    comment_count INTEGER DEFAULT 0,
                    published_at TIMESTAMP,
                    thumbnail_url TEXT,
                    rank INTEGER DEFAULT 0,
                    region VARCHAR(10) NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                );
                
                CREATE TABLE IF NOT EXISTS music_trends_cache (
                    id SERIAL PRIMARY KEY,
                    track_id VARCHAR(255) NOT NULL,
                    title TEXT NOT NULL,
                    artist VARCHAR(255) NOT NULL,
                    popularity INTEGER NOT NULL,
                    service VARCHAR(50) NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                );
                
                CREATE TABLE IF NOT EXISTS podcast_trends_cache (
                    id SERIAL PRIMARY KEY,
                    cache_key VARCHAR(255),
                    region_code VARCHAR(10),
                    podcast_id VARCHAR(255),
                    title TEXT NOT NULL,
                    description TEXT,
                    publisher VARCHAR(255),
                    url TEXT,
                    image_url TEXT,
                    language VARCHAR(10),
                    country VARCHAR(100),
                    score INTEGER DEFAULT 0,
                    rank INTEGER DEFAULT 0,
                    trend_type VARCHAR(50) NOT NULL,
                    region VARCHAR(10) NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                );
                
                CREATE TABLE IF NOT EXISTS news_trends_cache (
                    id SERIAL PRIMARY KEY,
                    article_id VARCHAR(255) NOT NULL,
                    title TEXT NOT NULL,
                    source VARCHAR(255) NOT NULL,
                    published_at TIMESTAMP NOT NULL,
                    category VARCHAR(50) NOT NULL,
                    country VARCHAR(10) NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                );
                
                CREATE TABLE IF NOT EXISTS worldnews_trends_cache (
                    id SERIAL PRIMARY KEY,
                    article_id VARCHAR(255) NOT NULL,
                    title TEXT NOT NULL,
                    source VARCHAR(255) NOT NULL,
                    published_at TIMESTAMP,
                    category VARCHAR(50) NOT NULL,
                    country VARCHAR(10) NOT NULL,
                    url TEXT,
                    description TEXT,
                    image_url TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                );
                
                CREATE TABLE IF NOT EXISTS rakuten_trends_cache (
                    id SERIAL PRIMARY KEY,
                    item_id VARCHAR(255) NOT NULL,
                    title TEXT NOT NULL,
                    price INTEGER NOT NULL,
                    category VARCHAR(100) NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                );
                
                CREATE TABLE IF NOT EXISTS hatena_trends_cache (
                    id SERIAL PRIMARY KEY,
                    entry_id VARCHAR(255) NOT NULL,
                    title TEXT NOT NULL,
                    url TEXT NOT NULL,
                    description TEXT,
                    bookmark_count INTEGER NOT NULL,
                    published VARCHAR(100),
                    author VARCHAR(255),
                    rank INTEGER DEFAULT 0,
                    category VARCHAR(50) NOT NULL,
                    region VARCHAR(50),
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                );
                
                CREATE TABLE IF NOT EXISTS twitch_trends_cache (
                    id SERIAL PRIMARY KEY,
                    title VARCHAR(500),
                    game_name VARCHAR(255),
                    viewer_count INTEGER DEFAULT 0,
                    rank INTEGER DEFAULT 0,
                    category VARCHAR(50) NOT NULL,
                    thumbnail_url VARCHAR(500),
                    user_name VARCHAR(255),
                    language VARCHAR(10),
                    started_at VARCHAR(50),
                    view_count INTEGER DEFAULT 0,
                    creator_name VARCHAR(255),
                    duration INTEGER DEFAULT 0,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    url VARCHAR(500),
                    box_art_url VARCHAR(500),
                    game_id VARCHAR(255)
                );
                
                CREATE TABLE IF NOT EXISTS reddit_trends_cache (
                    id SERIAL PRIMARY KEY,
                    post_id VARCHAR(255),
                    title TEXT NOT NULL,
                    url TEXT,
                    subreddit VARCHAR(100) NOT NULL,
                    author VARCHAR(100),
                    score INTEGER DEFAULT 0,
                    upvote_ratio FLOAT DEFAULT 0,
                    num_comments INTEGER DEFAULT 0,
                    permalink TEXT,
                    is_video BOOLEAN DEFAULT FALSE,
                    domain VARCHAR(255),
                    rank INTEGER DEFAULT 0,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    region VARCHAR(10) DEFAULT 'all'
                );
                
                CREATE TABLE IF NOT EXISTS hackernews_trends_cache (
                    id SERIAL PRIMARY KEY,
                    story_id INTEGER NOT NULL,
                    title TEXT NOT NULL,
                    url TEXT,
                    score INTEGER DEFAULT 0,
                    author VARCHAR(100),
                    story_time INTEGER,
                    comments INTEGER DEFAULT 0,
                    story_type VARCHAR(50) DEFAULT 'top',
                    rank INTEGER DEFAULT 0,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                );
                
                CREATE TABLE IF NOT EXISTS qiita_trends_cache (
                    id SERIAL PRIMARY KEY,
                    item_id VARCHAR(255) NOT NULL,
                    title TEXT NOT NULL,
                    url TEXT,
                    user_id VARCHAR(100),
                    user_name VARCHAR(100),
                    likes_count INTEGER DEFAULT 0,
                    stocks_count INTEGER DEFAULT 0,
                    comments_count INTEGER DEFAULT 0,
                    created_at VARCHAR(100),
                    updated_at VARCHAR(100),
                    tags TEXT,
                    rank INTEGER DEFAULT 0,
                    cached_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                );
                
                CREATE TABLE IF NOT EXISTS nhk_trends_cache (
                    id SERIAL PRIMARY KEY,
                    title TEXT NOT NULL,
                    url TEXT,
                    published_date TIMESTAMP WITH TIME ZONE,
                    description TEXT,
                    rank INTEGER DEFAULT 0,
                    cached_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                );
                
                CREATE TABLE IF NOT EXISTS cnn_trends_cache (
                    id SERIAL PRIMARY KEY,
                    title TEXT NOT NULL,
                    url TEXT,
                    published_date TIMESTAMP WITH TIME ZONE,
                    description TEXT,
                    rank INTEGER DEFAULT 0,
                    cached_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                );
                
                CREATE TABLE IF NOT EXISTS producthunt_trends_cache (
                    id SERIAL PRIMARY KEY,
                    product_id VARCHAR(255) NOT NULL,
                    name TEXT NOT NULL,
                    tagline TEXT,
                    description TEXT,
                    url TEXT,
                    website TEXT,
                    votes_count INTEGER DEFAULT 0,
                    comments_count INTEGER DEFAULT 0,
                    created_at VARCHAR(100),
                    topics TEXT,
                    user_name VARCHAR(100),
                    user_username VARCHAR(100),
                    rank INTEGER DEFAULT 0,
                    cached_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                );
                
                CREATE TABLE IF NOT EXISTS stock_trends_cache (
                    id SERIAL PRIMARY KEY,
                    symbol VARCHAR(50) NOT NULL,
                    name TEXT NOT NULL,
                    current_price DECIMAL(15, 2),
                    previous_price DECIMAL(15, 2),
                    change DECIMAL(15, 2),
                    change_percent DECIMAL(10, 2),
                    volume BIGINT DEFAULT 0,
                    market_cap BIGINT DEFAULT 0,
                    market VARCHAR(10) NOT NULL,
                    rank INTEGER DEFAULT 0,
                    updated_at TIMESTAMP,
                    cached_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                );
                
                CREATE TABLE IF NOT EXISTS crypto_trends_cache (
                    id SERIAL PRIMARY KEY,
                    coin_id VARCHAR(100) NOT NULL,
                    symbol VARCHAR(20) NOT NULL,
                    name TEXT NOT NULL,
                    market_cap_rank INTEGER DEFAULT 0,
                    search_score INTEGER DEFAULT 0,
                    current_price DECIMAL(20, 8),
                    price_change_24h DECIMAL(20, 8),
                    price_change_percentage_24h DECIMAL(10, 2),
                    market_cap BIGINT DEFAULT 0,
                    volume_24h BIGINT DEFAULT 0,
                    image_url TEXT,
                    rank INTEGER DEFAULT 0,
                    updated_at TIMESTAMP,
                    cached_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                );
                
                CREATE TABLE IF NOT EXISTS movie_trends_cache (
                    id SERIAL PRIMARY KEY,
                    country VARCHAR(10) NOT NULL DEFAULT 'JP',
                    movie_id INTEGER NOT NULL,
                    title VARCHAR(500) NOT NULL,
                    original_title VARCHAR(500),
                    overview TEXT,
                    popularity DECIMAL(10, 4),
                    vote_average DECIMAL(4, 2),
                    vote_count INTEGER,
                    release_date VARCHAR(20),
                    poster_path VARCHAR(500),
                    backdrop_path VARCHAR(500),
                    poster_url TEXT,
                    backdrop_url TEXT,
                    rank INTEGER,
                    updated_at TIMESTAMP,
                    cached_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                );
                
                CREATE TABLE IF NOT EXISTS book_trends_cache (
                    id SERIAL PRIMARY KEY,
                    country VARCHAR(10) NOT NULL,
                    book_id VARCHAR(100),
                    isbn VARCHAR(20),
                    title VARCHAR(500) NOT NULL,
                    subtitle VARCHAR(500),
                    author TEXT,
                    authors TEXT,
                    publisher VARCHAR(200),
                    price DECIMAL(10, 2),
                    sales INTEGER,
                    published_date VARCHAR(20),
                    release_date VARCHAR(20),
                    description TEXT,
                    page_count INTEGER,
                    categories TEXT,
                    average_rating DECIMAL(3, 2),
                    ratings_count INTEGER,
                    language VARCHAR(10),
                    item_url TEXT,
                    affiliate_url TEXT,
                    preview_link TEXT,
                    info_link TEXT,
                    buy_link TEXT,
                    image_url TEXT,
                    thumbnail TEXT,
                    small_thumbnail TEXT,
                    medium TEXT,
                    large TEXT,
                    rank INTEGER,
                    updated_at TIMESTAMP,
                    cached_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                );
                
                CREATE TABLE IF NOT EXISTS cache_status (
                    id SERIAL PRIMARY KEY,
                    cache_key VARCHAR(255) NOT NULL UNIQUE,
                    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    data_count INTEGER DEFAULT 0,
                    status VARCHAR(50) DEFAULT 'active'
                );
                
                -- æ—¢å­˜ã®country_codeã‚«ãƒ©ãƒ ã‚’cache_keyã«å¤‰æ›´ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
                DO $$ 
                BEGIN
                    IF EXISTS (SELECT 1 FROM information_schema.columns WHERE table_name = 'cache_status' AND column_name = 'country_code') THEN
                        ALTER TABLE cache_status RENAME COLUMN country_code TO cache_key;
                    END IF;
                END $$;
                
                CREATE TABLE IF NOT EXISTS subscriptions (
                    id SERIAL PRIMARY KEY,
                    email VARCHAR(255) NOT NULL UNIQUE,
                    subscribed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    is_active BOOLEAN DEFAULT TRUE
                );
                """
                
                cursor.execute(create_tables_sql)
                conn.commit()
                logger.info("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆå®Œäº†")
                return True
                
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return False
        except Exception as e:
            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return False
    
    def save_to_cache(self, data, cache_key, region='JP'):
        """ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        if not data:
            return False
        
        # Fly.ioã§ã¯æ¥ç¶šã‚’ä¿æŒã™ã‚‹ã‚ˆã‚Šã‚‚ã€æ¯å›æ–°è¦æ¥ç¶šã‚’ä½œæˆã™ã‚‹æ–¹ãŒå®‰å…¨
        # æ¥ç¶šã‚’å–å¾—ï¼ˆæ¯å›æ–°è¦æ¥ç¶šã‚’ä½œæˆã€ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ä»˜ãï¼‰
        import time
        conn = None
        max_retries = 3
        retry_delay = 1.0  # 1ç§’å¾…æ©Ÿã—ã¦ã‹ã‚‰å†è©¦è¡Œ
        
        for attempt in range(max_retries):
            try:
                # æ¥ç¶šã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦ã‹ã‚‰æ–°è¦æ¥ç¶šã‚’ä½œæˆ
                if self.connection:
                    try:
                        if not self.connection.closed:
                            self.connection.close()
                    except:
                        pass
                    self.connection = None
                
                # æ–°è¦æ¥ç¶šã‚’ä½œæˆ
                database_url = os.getenv('DATABASE_URL')
                if database_url:
                    conn = psycopg2.connect(
                        database_url,
                        connect_timeout=5,  # 5ç§’ã«çŸ­ç¸®ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã¯é«˜é€Ÿã§ã‚ã‚‹ã¹ãï¼‰
                        keepalives=1,
                        keepalives_idle=10,
                        keepalives_interval=5,
                        keepalives_count=5
                    )
                    conn.autocommit = False
                    logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šæˆåŠŸ (è©¦è¡Œ {attempt + 1}/{max_retries})")
                    break  # æ¥ç¶šæˆåŠŸã—ãŸã‚‰ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
                else:
                    conn = psycopg2.connect(
                        host=os.getenv('DB_HOST', 'localhost'),
                        port=os.getenv('DB_PORT', '5432'),
                        database=os.getenv('DB_NAME', 'trends_db'),
                        user=os.getenv('DB_USER', 'postgres'),
                        password=os.getenv('DB_PASSWORD', 'password'),
                        connect_timeout=10,
                        keepalives=1,
                        keepalives_idle=30,
                        keepalives_interval=10,
                        keepalives_count=3
                    )
                    conn.autocommit = False
                    logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šæˆåŠŸ (è©¦è¡Œ {attempt + 1}/{max_retries})")
                    break  # æ¥ç¶šæˆåŠŸã—ãŸã‚‰ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
            except (psycopg2.OperationalError, psycopg2.InterfaceError) as e:
                if attempt < max_retries - 1:
                    logger.warning(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå¤±æ•— (è©¦è¡Œ {attempt + 1}/{max_retries}): {e} - {retry_delay}ç§’å¾Œã«å†è©¦è¡Œã—ã¾ã™", exc_info=True)
                    time.sleep(retry_delay)
                    retry_delay *= 1.5  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
                    continue
                else:
                    logger.error(f"âŒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆæœ€å¤§è©¦è¡Œå›æ•°ã«é”ã—ã¾ã—ãŸï¼‰: {e}", exc_info=True)
                    return False
            except Exception as e:
                logger.error(f"âŒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
                import traceback
                traceback.print_exc()
                return False
        
        if not conn:
            logger.error("âŒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return False
        
        try:
            with conn.cursor() as cursor:
                # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤
                table_name = f"{cache_key}_cache"
                delete_column_map = {
                    'google_trends': 'region',
                    'podcast_trends': 'region',
                    'news_trends': 'country',
                    'worldnews_trends': 'country',
                    'rakuten_trends': 'genre_id',
                    'hatena_trends': 'category',
                    'twitch_trends': 'category'
                }
                
                if cache_key == 'music_trends':
                    cursor.execute(f"DELETE FROM {table_name} WHERE service = %s", (region,))
                elif cache_key == 'podcast_trends':
                    cursor.execute(f"DELETE FROM podcast_trends_cache WHERE region = %s", (region,))
                elif cache_key == 'youtube_trends':
                    # YouTubeã¯regionã¨trend_typeã§å‰Šé™¤
                    # dataã®æœ€åˆã®itemã‹ã‚‰trend_typeã‚’å–å¾—
                    trend_type = data[0].get('trend_type', 'trending') if data else 'trending'
                    cursor.execute(f"DELETE FROM {table_name} WHERE region = %s AND trend_type = %s", (region, trend_type))
                elif cache_key == 'reddit_trends':
                    cursor.execute(f"DELETE FROM {table_name} WHERE subreddit = %s", (region,))
                elif cache_key == 'hackernews_trends':
                    cursor.execute(f"DELETE FROM {table_name} WHERE story_type = %s", (region,))
                elif cache_key == 'qiita_trends':
                    cursor.execute(f"DELETE FROM {table_name}")
                elif cache_key == 'nhk_trends':
                    cursor.execute(f"DELETE FROM {table_name}")
                elif cache_key == 'producthunt_trends':
                    cursor.execute(f"DELETE FROM {table_name}")
                elif cache_key == 'hatena_trends':
                    # hatena_trendsã®å ´åˆã¯categoryã§å‰Šé™¤
                    if region and region != '':
                        cursor.execute(f"DELETE FROM {table_name} WHERE category = %s", (region,))
                    else:
                        # regionãŒç©ºã®å ´åˆã¯å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤
                        cursor.execute(f"DELETE FROM {table_name}")
                else:
                    delete_column = delete_column_map.get(cache_key, 'region')
                    if delete_column and region is not None:
                        cursor.execute(f"DELETE FROM {table_name} WHERE {delete_column} = %s", (region,))
                    else:
                        cursor.execute(f"DELETE FROM {table_name}")
                
                # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’æŒ¿å…¥
                for item in data:
                    if cache_key == 'google_trends':
                        cursor.execute(
                            "INSERT INTO google_trends_cache (keyword, score, region) VALUES (%s, %s, %s)",
                            (item.get('keyword', ''), item.get('score', 0), region)
                        )
                    elif cache_key == 'youtube_trends':
                        cursor.execute(
                            "INSERT INTO youtube_trends_cache (region_code, trend_type, video_id, title, channel_title, view_count, like_count, comment_count, published_at, thumbnail_url, rank, region) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)",
                            (region, item.get('trend_type', 'trending'), item.get('video_id', ''), item.get('title', ''), item.get('channel_title', ''), item.get('view_count', 0), item.get('like_count', 0), item.get('comment_count', 0), item.get('published_at') or None, item.get('thumbnail_url', ''), item.get('rank', 0), region)
                        )
                    elif cache_key == 'music_trends':
                        cursor.execute(
                            "INSERT INTO music_trends_cache (service, region_code, title, artist, album, play_count, popularity, spotify_url, rank, track_id) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)",
                            (item.get('service', 'spotify'), region, item.get('title', ''), item.get('artist', ''), item.get('album', ''), item.get('play_count', 0), item.get('popularity', 0), item.get('spotify_url', ''), item.get('rank', 0), item.get('track_id', ''))
                        )
                    elif cache_key == 'podcast_trends':
                        # podcast_idã¯itemã®idã¾ãŸã¯podcast_idã¾ãŸã¯listennotes_urlã‹ã‚‰æŠ½å‡º
                        podcast_id = item.get('id', '') or item.get('podcast_id', '')
                        if not podcast_id and item.get('listennotes_url'):
                            # listennotes_urlã‹ã‚‰IDã‚’æŠ½å‡º: https://www.listennotes.com/c/{id}/
                            url_parts = item.get('listennotes_url', '').rstrip('/').split('/')
                            podcast_id = url_parts[-1] if url_parts else ''
                        # podcast_idãŒç©ºã®å ´åˆã¯ã€ã‚¿ã‚¤ãƒˆãƒ«ã¨publisherã‹ã‚‰ç”Ÿæˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
                        if not podcast_id:
                            podcast_id = f"{item.get('title', '')[:50]}_{item.get('publisher', '')[:30]}".replace(' ', '_')[:100]
                        cursor.execute(
                            "INSERT INTO podcast_trends_cache (podcast_id, cache_key, region_code, title, description, publisher, url, image_url, language, country, score, rank, trend_type, region) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)",
                            (podcast_id, cache_key, region, item.get('title', ''), item.get('description', ''), item.get('publisher', ''), item.get('url', ''), item.get('image_url', ''), item.get('language', ''), item.get('country', ''), item.get('score', 0), item.get('rank', 0), item.get('trend_type', ''), region)
                        )
                    elif cache_key == 'news_trends':
                        cursor.execute(
                            "INSERT INTO news_trends_cache (article_id, title, source, published_at, category, country) VALUES (%s, %s, %s, %s, %s, %s)",
                            (item.get('article_id', ''), item.get('title', ''), item.get('source', ''), item.get('published_at', ''), item.get('category', ''), item.get('country', ''))
                        )
                    elif cache_key == 'worldnews_trends':
                        cursor.execute(
                            "INSERT INTO worldnews_trends_cache (article_id, title, source, published_at, category, country, url, description, image_url) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)",
                            (
                                item.get('article_id', ''),
                                item.get('title', ''),
                                item.get('source', ''),
                                item.get('published_at') or None,
                                item.get('category', ''),
                                item.get('country', ''),
                                item.get('url', ''),
                                item.get('description', ''),
                                item.get('image_url', '')
                            )
                        )
                    elif cache_key == 'rakuten_trends':
                        # rakuten_trendsã®å ´åˆã€genre_idã‚«ãƒ©ãƒ ã«ã¯ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚ã®genre_idï¼ˆregionãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰ã‚’ä¿å­˜
                        # å„ã‚¢ã‚¤ãƒ†ãƒ ã®genreIdã§ã¯ãªãã€ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚ã®genre_idã‚’ä½¿ç”¨
                        # item_idã¯itemCodeã‹ã‚‰å–å¾—ï¼ˆä¾‹: 'alpen:10499596'ï¼‰
                        item_id = item.get('item_id', '') or item.get('itemCode', '')
                        if not item_id:
                            # item_idãŒå–å¾—ã§ããªã„å ´åˆã¯ã€URLã‹ã‚‰ç”Ÿæˆã™ã‚‹ã‹ã€ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹
                            logger.warning(f"âš ï¸ Rakuten: item_idãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚item keys: {list(item.keys())}")
                            continue
                        # ãƒ‡ãƒãƒƒã‚°: æœ€åˆã®ã‚¢ã‚¤ãƒ†ãƒ ã®ã¿ãƒ­ã‚°å‡ºåŠ›
                        if data.index(item) == 0:
                            logger.debug(f"ğŸ” Rakuten: item_id={item_id}, genre_id={region}, title={item.get('title', '')[:30]}")
                        cursor.execute(
                            "INSERT INTO rakuten_trends_cache (item_id, genre_id, title, price, category, review_count, review_average, image_url, url, shop_name, sales_rank, sales_count, rank, region) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)",
                            (item_id, region, item.get('title', ''), item.get('price', 0), region or 'all', item.get('review_count', 0), item.get('review_average', 0.0), item.get('image_url', ''), item.get('url', ''), item.get('shop_name', ''), item.get('sales_rank', ''), item.get('sales_count', ''), item.get('rank', 0), region)
                        )
                    elif cache_key == 'hatena_trends':
                        cursor.execute(
                            "INSERT INTO hatena_trends_cache (category, title, url, description, bookmark_count, published, author, rank, region, entry_id) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)",
                            (item.get('category', ''), item.get('title', ''), item.get('url', ''), item.get('description', ''), item.get('bookmark_count', 0), item.get('published', ''), item.get('author', ''), item.get('rank', 0), region, item.get('entry_id', ''))
                        )
                    elif cache_key == 'twitch_trends':
                        cursor.execute(
                            "INSERT INTO twitch_trends_cache (category, title, game_name, viewer_count, view_count, user_name, creator_name, thumbnail_url, url, rank, box_art_url, game_id) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)",
                            (region, item.get('title', ''), item.get('game_name', '') or item.get('name', ''), item.get('viewer_count', 0), item.get('view_count', 0), item.get('user_name', ''), item.get('creator_name', ''), item.get('thumbnail_url', ''), item.get('url', ''), item.get('rank', 0), item.get('box_art_url', ''), item.get('id', '') or item.get('game_id', ''))
                        )
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ›´æ–°
                cursor.execute(
                    "INSERT INTO cache_status (cache_key, last_updated, data_count) VALUES (%s, %s, %s) ON CONFLICT (cache_key) DO UPDATE SET last_updated = %s, data_count = %s",
                    (cache_key, datetime.now(), len(data), datetime.now(), len(data))
                )
                
                conn.commit()
                logger.info(f"âœ… {cache_key}ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–°ã—ã¾ã—ãŸ ({len(data)}ä»¶)")
                # æ¥ç¶šã‚’é–‰ã˜ã‚‹ï¼ˆæ¯å›æ–°è¦æ¥ç¶šã‚’ä½œæˆã™ã‚‹ãŸã‚ï¼‰
                try:
                    if conn and not conn.closed:
                        conn.close()
                except:
                    pass
                return True
                
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            # æ¥ç¶šã‚¨ãƒ©ãƒ¼ã®å ´åˆ
            logger.warning(f"âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            import traceback
            traceback.print_exc()
            # ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è©¦ã¿ã‚‹ï¼ˆæ¥ç¶šãŒæœ‰åŠ¹ãªå ´åˆã®ã¿ï¼‰
            try:
                if conn and not conn.closed:
                    conn.rollback()
                    conn.close()
            except:
                pass
            return False
        except Exception as e:
            logger.error(f"âŒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            import traceback
            traceback.print_exc()
            # ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è©¦ã¿ã‚‹ï¼ˆæ¥ç¶šãŒæœ‰åŠ¹ãªå ´åˆã®ã¿ï¼‰
            try:
                if conn and not conn.closed:
                    conn.rollback()
                    conn.close()
            except:
                pass
            return False
    
    def get_from_cache(self, cache_key, region='JP'):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        # æ¥ç¶šã‚’å–å¾—ï¼ˆæœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯ã¨å†æ¥ç¶šã‚’è‡ªå‹•ã§è¡Œã†ï¼‰
        try:
            conn = self.get_connection()
            if not conn:
                logger.error("âŒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                return None
        except Exception as e:
            logger.error(f"âŒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return None
        
        try:
            with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                table_name = f"{cache_key}_cache"
                
                # hatena_trendsã¨twitch_trendsã®å ´åˆã¯categoryã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if cache_key == 'hatena_trends':
                    if region and region != '':
                        cursor.execute(f"SELECT * FROM {table_name} WHERE category = %s ORDER BY rank ASC, created_at DESC", (region,))
                    else:
                        cursor.execute(f"SELECT * FROM {table_name} ORDER BY rank ASC, created_at DESC")
                elif cache_key == 'twitch_trends':
                    if region and region != '':
                        cursor.execute(f"SELECT * FROM {table_name} WHERE category = %s ORDER BY rank ASC, created_at DESC", (region,))
                    else:
                        cursor.execute(f"SELECT * FROM {table_name} ORDER BY rank ASC, created_at DESC")
                # rakuten_trendsã®å ´åˆã¯genre_idã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆregionãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒgenre_idã¨ã—ã¦æ¸¡ã•ã‚Œã‚‹ï¼‰
                elif cache_key == 'rakuten_trends':
                    if region and region != '':
                        cursor.execute(f"SELECT * FROM {table_name} WHERE genre_id = %s ORDER BY rank ASC, created_at DESC", (region,))
                    else:
                        cursor.execute(f"SELECT * FROM {table_name} ORDER BY rank ASC, created_at DESC")
                # regionãŒç©ºã®å ´åˆã¯regionæ¡ä»¶ã‚’é™¤å¤–
                elif region and region != '':
                    cursor.execute(f"SELECT * FROM {table_name} WHERE region = %s ORDER BY created_at DESC", (region,))
                else:
                    cursor.execute(f"SELECT * FROM {table_name} ORDER BY created_at DESC")
                
                data = cursor.fetchall()
                
                # RealDictCursorã®çµæœã‚’è¾æ›¸ã®ãƒªã‚¹ãƒˆã«å¤‰æ›
                result = []
                for row in data:
                    result.append(dict(row))
                
                return result
                
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            # æ¥ç¶šã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯å†æ¥ç¶šã‚’è©¦ã¿ã‚‹
            logger.warning(f"âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            try:
                # æ¥ç¶šã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦æ¬¡å›ã®æ“ä½œã§å†æ¥ç¶šã•ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹
                self.connection = None
                return None
            except Exception as retry_error:
                logger.error(f"âŒ å†æ¥ç¶šè©¦è¡Œã‚¨ãƒ©ãƒ¼: {retry_error}", exc_info=True)
                return None
        except Exception as e:
            logger.error(f"âŒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            import traceback
            traceback.print_exc()
            return None
    
    def is_cache_valid(self, cache_key, region='JP', hours=24):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ã‹ã©ã†ã‹ã‚’ç¢ºèª"""
        # æ¥ç¶šã‚’å–å¾—ï¼ˆæœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯ã¨å†æ¥ç¶šã‚’è‡ªå‹•ã§è¡Œã†ï¼‰
        try:
            conn = self.get_connection()
            if not conn:
                return False
        except Exception as e:
            logger.warning(f"âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return False
        
        try:
            with conn.cursor() as cursor:
                cursor.execute(
                    "SELECT last_updated FROM cache_status WHERE cache_key = %s",
                    (cache_key,)
                )
                result = cursor.fetchone()
                
                if not result:
                    return False
                
                last_updated = result[0]
                now = datetime.now()
                time_diff = now - last_updated
                
                return time_diff.total_seconds() < (hours * 3600)
                
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            # æ¥ç¶šã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯å†æ¥ç¶šã‚’è©¦ã¿ã‚‹
            logger.warning(f"âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            try:
                # æ¥ç¶šã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦æ¬¡å›ã®æ“ä½œã§å†æ¥ç¶šã•ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹
                self.connection = None
                return False
            except Exception as retry_error:
                logger.error(f"âŒ å†æ¥ç¶šè©¦è¡Œã‚¨ãƒ©ãƒ¼: {retry_error}", exc_info=True)
                return False
        except Exception as e:
            logger.error(f"âŒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹æ€§ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return False
    
    def clear_cache(self, cache_key, region='JP'):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        try:
            conn = self.get_connection()
            if not conn:
                return False
        except Exception as e:
            logger.error(f"âŒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return False
        
        try:
            with conn.cursor() as cursor:
                table_name = f"{cache_key}_cache"
                cursor.execute(f"DELETE FROM {table_name} WHERE region = %s", (region,))
                conn.commit()
                logger.info(f"âœ… {cache_key}ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
                return True
                
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return False
        except Exception as e:
            logger.error(f"âŒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return False
    
    def get_connection(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’å–å¾—ï¼ˆæ¥ç¶šã•ã‚Œã¦ã„ãªã„å ´åˆã¯å†æ¥ç¶šã‚’è©¦ã¿ã‚‹ï¼‰

        ãƒ­ãƒƒã‚¯ã‚’ä½¿ç”¨ã—ã¦ã€è¤‡æ•°ã®ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‹ã‚‰ã®åŒæ™‚ã‚¢ã‚¯ã‚»ã‚¹ã‚’é˜²ã
        ãƒ­ãƒƒã‚¯ã®ç¯„å›²ã‚’æœ€å°é™ã«ã—ã¦ã€ã‚¯ã‚¨ãƒªå®Ÿè¡Œæ™‚ã®ãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ã‚’é˜²ã
        """
        import time
        global _connection_lock

        max_retries = 3  # ãƒªãƒˆãƒ©ã‚¤å›æ•°ã‚’3ã«å¢—åŠ ï¼ˆä¸¦åˆ—ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚ã®æ¥ç¶šç¢ºç«‹ã‚’è€ƒæ…®ï¼‰
        retry_delay = 0.5  # å¾…æ©Ÿæ™‚é–“ã‚’0.5ç§’ã«è¨­å®šï¼ˆæ¥ç¶šç¢ºç«‹ã®ä½™è£•ã‚’æŒãŸã›ã‚‹ï¼‰

        # æ¥ç¶šãŒæ—¢ã«å­˜åœ¨ã—ã€é–‰ã˜ã‚‰ã‚Œã¦ã„ãªã„å ´åˆã¯å³åº§ã«è¿”ã™ï¼ˆãƒ­ãƒƒã‚¯ä¸è¦ã€SELECT 1ãƒã‚§ãƒƒã‚¯ã‚‚çœç•¥ï¼‰
        # æ¥ç¶šãŒç„¡åŠ¹ãªå ´åˆã¯ã€ã‚¯ã‚¨ãƒªå®Ÿè¡Œæ™‚ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ã®ã§ã€ãã®æ™‚ç‚¹ã§å†æ¥ç¶šã™ã‚‹
        if self.connection and not self.connection.closed:
            return self.connection

        # ãƒ­ãƒƒã‚¯ã‚’å–å¾—ã—ã¦ã€åŒæ™‚ã‚¢ã‚¯ã‚»ã‚¹ã‚’é˜²ãï¼ˆå†æ¥ç¶šæ™‚ã®ã¿ï¼‰
        with _connection_lock:
            # ãƒ­ãƒƒã‚¯å–å¾—å¾Œã€å†åº¦ãƒã‚§ãƒƒã‚¯ï¼ˆä»–ã®ã‚¹ãƒ¬ãƒƒãƒ‰ãŒæ—¢ã«æ¥ç¶šã‚’ç¢ºç«‹ã—ãŸå¯èƒ½æ€§ãŒã‚ã‚‹ï¼‰
            if self.connection and not self.connection.closed:
                return self.connection

            for attempt in range(max_retries):
                try:
                    # æ¥ç¶šãŒå­˜åœ¨ã—ãªã„ã€ã¾ãŸã¯é–‰ã˜ã‚‰ã‚Œã¦ã„ã‚‹å ´åˆã¯å†æ¥ç¶š
                    if not self.connection:
                        self.connect()
                    else:
                        try:
                            # æ¥ç¶šãŒé–‰ã˜ã‚‰ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
                            if self.connection.closed:
                                self.connect()
                            # æ¥ç¶šãŒæœ‰åŠ¹ã‹ã©ã†ã‹ã¯ã€å®Ÿéš›ã®ã‚¯ã‚¨ãƒªå®Ÿè¡Œæ™‚ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸã‚‰å†æ¥ç¶šã™ã‚‹
                            # SELECT 1ã«ã‚ˆã‚‹äº‹å‰ç¢ºèªã¯å‰Šé™¤ï¼ˆãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ã®åŸå› ã¨ãªã‚‹ãŸã‚ï¼‰
                        except (psycopg2.InterfaceError, psycopg2.OperationalError, AttributeError) as e:
                            # æ¥ç¶šã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯å†æ¥ç¶š
                            logger.warning(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚¨ãƒ©ãƒ¼æ¤œå‡ºã€å†æ¥ç¶šã—ã¾ã™: {e}")
                            self.connect()
                    
                    # æ¥ç¶šãŒç¢ºç«‹ã•ã‚ŒãŸã‹ç¢ºèªï¼ˆSELECT 1ã«ã‚ˆã‚‹ç¢ºèªã¯å‰Šé™¤ï¼‰
                    if self.connection and not self.connection.closed:
                        # æ¥ç¶šãŒæœ‰åŠ¹ã‹ã©ã†ã‹ã¯ã€å®Ÿéš›ã®ã‚¯ã‚¨ãƒªå®Ÿè¡Œæ™‚ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸã‚‰å†æ¥ç¶šã™ã‚‹
                        # äº‹å‰ç¢ºèªã‚’å‰Šé™¤ã™ã‚‹ã“ã¨ã§ã€ãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ã‚’é˜²ã
                        return self.connection
                except Exception as e:
                    # äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ã®å ´åˆã‚‚å†è©¦è¡Œ
                    if attempt < max_retries - 1:
                        logger.warning(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã‚¨ãƒ©ãƒ¼: {e} - {retry_delay}ç§’å¾Œã«å†è©¦è¡Œã—ã¾ã™")
                        time.sleep(retry_delay)
                        self.connection = None
                        continue
                    else:
                        logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã‚¨ãƒ©ãƒ¼ï¼ˆæœ€å¤§è©¦è¡Œå›æ•°ï¼‰: {e}")
            
            # å…¨ã¦ã®å†è©¦è¡ŒãŒå¤±æ•—ã—ãŸå ´åˆ
            error_msg = "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’ç¢ºç«‹ã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆæœ€å¤§è©¦è¡Œå›æ•°ã«é”ã—ã¾ã—ãŸï¼‰"
            logger.error(f"âŒ {error_msg}")
            raise psycopg2.OperationalError(error_msg)
    
    # Google Trends ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ã‚½ãƒƒãƒ‰
    def save_google_trends_to_cache(self, data, region='JP'):
        """Google Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        return self.save_to_cache(data, 'google_trends', region)
    
    def get_google_trends_from_cache(self, region='JP'):
        """Google Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—"""
        return self.get_from_cache('google_trends', region)
    
    def clear_google_trends_cache(self, region='JP'):
        """Google Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        return self.clear_cache('google_trends', region)
    
    def is_google_cache_valid(self, region='JP'):
        """Google Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ã‹ã©ã†ã‹ã‚’ç¢ºèª"""
        return self.is_cache_valid('google_trends', region, 6)
    
    # YouTube Trends ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ã‚½ãƒƒãƒ‰
    def save_youtube_trends_to_cache(self, data, region='JP', trend_type='trending'):
        """YouTube Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        if not data:
            return False
        
        # trend_typeã‚’å„itemã«è¿½åŠ ï¼ˆè¾æ›¸ã®å ´åˆã®ã¿ï¼‰
        if isinstance(data, list) and isinstance(data[0], dict):
            for item in data:
                if not item.get('trend_type'):
                    item['trend_type'] = trend_type
        
        # æ—¢å­˜ç’°å¢ƒã§ã¯youtube_trends_cacheã«ä¸è¶³ã‚«ãƒ©ãƒ ãŒã‚ã‚‹ã‚±ãƒ¼ã‚¹ãŒã‚ã‚‹ãŸã‚ã€ã“ã“ã§è£œå®Œã—ã¦ãŠã
        try:
            conn = self.get_connection()
            if conn:
                with conn.cursor() as cursor:
                    # INSERTæ–‡ã§ä½¿ç”¨ã—ã¦ã„ã‚‹ã‚«ãƒ©ãƒ ã‚’ç¢ºèªã—ã¦è¿½åŠ 
                    cursor.execute("ALTER TABLE youtube_trends_cache ADD COLUMN IF NOT EXISTS region_code VARCHAR(10)")
                    cursor.execute("ALTER TABLE youtube_trends_cache ADD COLUMN IF NOT EXISTS trend_type VARCHAR(50) DEFAULT 'trending'")
                    cursor.execute("ALTER TABLE youtube_trends_cache ADD COLUMN IF NOT EXISTS video_id VARCHAR(255)")
                    cursor.execute("ALTER TABLE youtube_trends_cache ADD COLUMN IF NOT EXISTS like_count INTEGER DEFAULT 0")
                    cursor.execute("ALTER TABLE youtube_trends_cache ADD COLUMN IF NOT EXISTS comment_count INTEGER DEFAULT 0")
                    cursor.execute("ALTER TABLE youtube_trends_cache ADD COLUMN IF NOT EXISTS published_at TIMESTAMP")
                    cursor.execute("ALTER TABLE youtube_trends_cache ADD COLUMN IF NOT EXISTS thumbnail_url TEXT")
                    cursor.execute("ALTER TABLE youtube_trends_cache ADD COLUMN IF NOT EXISTS rank INTEGER DEFAULT 0")
                    cursor.execute("ALTER TABLE youtube_trends_cache ADD COLUMN IF NOT EXISTS region VARCHAR(10)")
                    cursor.execute("ALTER TABLE youtube_trends_cache ADD COLUMN IF NOT EXISTS description TEXT")
                conn.commit()
        except Exception as e:
            logger.warning(f"âš ï¸ youtube_trends_cacheã®ã‚¹ã‚­ãƒ¼ãƒæ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
        
        return self.save_to_cache(data, 'youtube_trends', region)
    
    def get_youtube_trends_from_cache(self, region='JP', trend_type='trending'):
        """YouTube Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—"""
        # æ¥ç¶šã‚’å–å¾—ï¼ˆæœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯ã¨å†æ¥ç¶šã‚’è‡ªå‹•ã§è¡Œã†ï¼‰
        try:
            conn = self.get_connection()
            if not conn:
                return None
        except Exception as e:
            logger.error(f"âŒ YouTubeã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return None
        
        try:
            with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                cursor.execute(
                    "SELECT * FROM youtube_trends_cache WHERE region = %s AND trend_type = %s ORDER BY created_at DESC",
                    (region, trend_type)
                )
                data = cursor.fetchall()
                
                # RealDictCursorã®çµæœã‚’è¾æ›¸ã®ãƒªã‚¹ãƒˆã«å¤‰æ›
                result = []
                for row in data:
                    result.append(dict(row))
                
                return result
                
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ YouTubeã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return None
        except Exception as e:
            logger.error(f"âŒ YouTubeã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return None
    
    def clear_youtube_trends_cache(self, region='JP'):
        """YouTube Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        return self.clear_cache('youtube_trends', region)
    
    def is_youtube_cache_valid(self, region='JP'):
        """YouTube Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ã‹ã©ã†ã‹ã‚’ç¢ºèª"""
        return self.is_cache_valid('youtube_trends', region, 6)
    
    # Music Trends ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ã‚½ãƒƒãƒ‰
    def save_music_trends_to_cache(self, data, service='spotify', region='JP'):
        """Music Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        if not data:
            return False
        
        # æ—¢å­˜ç’°å¢ƒã§ã¯music_trends_cacheã«ä¸è¶³ã‚«ãƒ©ãƒ ãŒã‚ã‚‹ã‚±ãƒ¼ã‚¹ãŒã‚ã‚‹ãŸã‚ã€ã“ã“ã§è£œå®Œã—ã¦ãŠã
        try:
            conn = self.get_connection()
            if conn:
                with conn.cursor() as cursor:
                    # INSERTæ–‡ã§ä½¿ç”¨ã—ã¦ã„ã‚‹ã‚«ãƒ©ãƒ ã‚’ç¢ºèªã—ã¦è¿½åŠ 
                    cursor.execute("ALTER TABLE music_trends_cache ADD COLUMN IF NOT EXISTS album TEXT")
                    cursor.execute("ALTER TABLE music_trends_cache ADD COLUMN IF NOT EXISTS play_count INTEGER DEFAULT 0")
                    cursor.execute("ALTER TABLE music_trends_cache ADD COLUMN IF NOT EXISTS spotify_url TEXT")
                    cursor.execute("ALTER TABLE music_trends_cache ADD COLUMN IF NOT EXISTS rank INTEGER DEFAULT 0")
                    cursor.execute("ALTER TABLE music_trends_cache ADD COLUMN IF NOT EXISTS region_code VARCHAR(10)")
                    cursor.execute("ALTER TABLE music_trends_cache ADD COLUMN IF NOT EXISTS track_id VARCHAR(255)")
                conn.commit()
        except Exception as e:
            logger.warning(f"âš ï¸ music_trends_cacheã®ã‚¹ã‚­ãƒ¼ãƒæ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
        
        # æ¥ç¶šã‚’å–å¾—ï¼ˆæœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯ã¨å†æ¥ç¶šã‚’è‡ªå‹•ã§è¡Œã†ï¼‰
        try:
            conn = self.get_connection()
            if not conn:
                return False
        except Exception as e:
            logger.error(f"âŒ music_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return False
        
        try:
            with conn.cursor() as cursor:
                # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤
                cursor.execute("""
                    DELETE FROM music_trends_cache 
                    WHERE service = %s AND region_code = %s
                """, (service, region))
                
                # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’æŒ¿å…¥
                for item in data:
                    cursor.execute("""
                        INSERT INTO music_trends_cache 
                        (title, artist, album, play_count, popularity, spotify_url, rank, 
                         service, region_code, created_at, track_id)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    """, (
                        item.get('title', ''),
                        item.get('artist', ''),
                        item.get('album', ''),
                        item.get('play_count', 0),
                        item.get('popularity', 0),
                        item.get('spotify_url', ''),
                        item.get('rank', 0),
                        service,
                        region,
                        item.get('created_at'),
                        item.get('track_id', '')
                    ))
                
                conn.commit()
                logger.info(f"âœ… music_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜ã—ã¾ã—ãŸ (service: {service}, region: {region}, {len(data)}ä»¶)")
                return True
                
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ music_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return False
        except Exception as e:
            logger.error(f"âŒ music_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            try:
                conn.rollback()
            except:
                pass
            return False
    
    def get_music_trends_from_cache(self, service='spotify', region='JP'):
        """Music Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—"""
        # æ—¢å­˜ç’°å¢ƒã§ã¯music_trends_cacheã«ä¸è¶³ã‚«ãƒ©ãƒ ãŒã‚ã‚‹ã‚±ãƒ¼ã‚¹ãŒã‚ã‚‹ãŸã‚ã€ã“ã“ã§è£œå®Œã—ã¦ãŠã
        try:
            conn = self.get_connection()
            if conn:
                with conn.cursor() as cursor:
                    # INSERTæ–‡ã§ä½¿ç”¨ã—ã¦ã„ã‚‹ã‚«ãƒ©ãƒ ã‚’ç¢ºèªã—ã¦è¿½åŠ 
                    cursor.execute("ALTER TABLE music_trends_cache ADD COLUMN IF NOT EXISTS album TEXT")
                    cursor.execute("ALTER TABLE music_trends_cache ADD COLUMN IF NOT EXISTS play_count INTEGER DEFAULT 0")
                    cursor.execute("ALTER TABLE music_trends_cache ADD COLUMN IF NOT EXISTS spotify_url TEXT")
                    cursor.execute("ALTER TABLE music_trends_cache ADD COLUMN IF NOT EXISTS rank INTEGER DEFAULT 0")
                    cursor.execute("ALTER TABLE music_trends_cache ADD COLUMN IF NOT EXISTS region_code VARCHAR(10)")
                    cursor.execute("ALTER TABLE music_trends_cache ADD COLUMN IF NOT EXISTS track_id VARCHAR(255)")
                conn.commit()
        except Exception as e:
            logger.warning(f"âš ï¸ music_trends_cacheã®ã‚¹ã‚­ãƒ¼ãƒæ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
        
        # æ¥ç¶šã‚’å–å¾—ï¼ˆæœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯ã¨å†æ¥ç¶šã‚’è‡ªå‹•ã§è¡Œã†ï¼‰
        try:
            conn = self.get_connection()
            if not conn:
                return None
        except Exception as e:
            logger.error(f"âŒ Music Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return None
        
        try:
            with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                cursor.execute("""
                    SELECT title, artist, album, play_count, popularity, spotify_url, rank, 
                           service, region_code, created_at, track_id
                    FROM music_trends_cache 
                    WHERE service = %s AND region_code = %s
                    ORDER BY rank
                """, (service, region))
                data = cursor.fetchall()
                
                # RealDictCursorã®çµæœã‚’è¾æ›¸ã®ãƒªã‚¹ãƒˆã«å¤‰æ›
                result = []
                for row in data:
                    result.append(dict(row))
                
                return result
                
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ Music Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return None
        except Exception as e:
            logger.error(f"âŒ Music Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return None
    
    def clear_music_trends_cache(self, service='spotify', region='JP'):
        """Music Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        # æ—¢å­˜ç’°å¢ƒã§ã¯music_trends_cacheã«ä¸è¶³ã‚«ãƒ©ãƒ ãŒã‚ã‚‹ã‚±ãƒ¼ã‚¹ãŒã‚ã‚‹ãŸã‚ã€ã“ã“ã§è£œå®Œã—ã¦ãŠã
        try:
            conn = self.get_connection()
            if conn:
                with conn.cursor() as cursor:
                    # INSERTæ–‡ã§ä½¿ç”¨ã—ã¦ã„ã‚‹ã‚«ãƒ©ãƒ ã‚’ç¢ºèªã—ã¦è¿½åŠ 
                    cursor.execute("ALTER TABLE music_trends_cache ADD COLUMN IF NOT EXISTS album TEXT")
                    cursor.execute("ALTER TABLE music_trends_cache ADD COLUMN IF NOT EXISTS play_count INTEGER DEFAULT 0")
                    cursor.execute("ALTER TABLE music_trends_cache ADD COLUMN IF NOT EXISTS spotify_url TEXT")
                    cursor.execute("ALTER TABLE music_trends_cache ADD COLUMN IF NOT EXISTS rank INTEGER DEFAULT 0")
                    cursor.execute("ALTER TABLE music_trends_cache ADD COLUMN IF NOT EXISTS region_code VARCHAR(10)")
                    cursor.execute("ALTER TABLE music_trends_cache ADD COLUMN IF NOT EXISTS track_id VARCHAR(255)")
                conn.commit()
        except Exception as e:
            logger.warning(f"âš ï¸ music_trends_cacheã®ã‚¹ã‚­ãƒ¼ãƒæ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
        
        try:
            conn = self.get_connection()
            if not conn:
                return False
        except Exception as e:
            logger.error(f"âŒ music_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return False
        
        try:
            with conn.cursor() as cursor:
                cursor.execute("""
                    DELETE FROM music_trends_cache 
                    WHERE service = %s AND region_code = %s
                """, (service, region))
                conn.commit()
                logger.info(f"âœ… music_trendsã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ (service: {service}, region: {region})")
                return True
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ music_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return False
        except Exception as e:
            logger.error(f"âŒ music_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            try:
                conn.rollback()
            except:
                pass
            return False
    
    def is_music_cache_valid(self, service='spotify'):
        """Music Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ã‹ã©ã†ã‹ã‚’ç¢ºèª"""
        return self.is_cache_valid('music_trends', service, 12)
    
    # Podcast Trends ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ã‚½ãƒƒãƒ‰
    def save_podcast_trends_to_cache(self, data, cache_key='podcast_trends', region='JP'):
        """Podcast Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        if not data:
            return False
        
        # æ—¢å­˜ç’°å¢ƒã§ã¯podcast_trends_cacheã«ä¸è¶³ã‚«ãƒ©ãƒ ãŒã‚ã‚‹ã‚±ãƒ¼ã‚¹ãŒã‚ã‚‹ãŸã‚ã€ã“ã“ã§è£œå®Œã—ã¦ãŠã
        try:
            conn = self.get_connection()
            if conn:
                with conn.cursor() as cursor:
                    # INSERTæ–‡ã§ä½¿ç”¨ã—ã¦ã„ã‚‹ã‚«ãƒ©ãƒ ã‚’ç¢ºèªã—ã¦è¿½åŠ 
                    cursor.execute("ALTER TABLE podcast_trends_cache ADD COLUMN IF NOT EXISTS cache_key VARCHAR(255)")
                    cursor.execute("ALTER TABLE podcast_trends_cache ADD COLUMN IF NOT EXISTS region_code VARCHAR(10)")
                    cursor.execute("ALTER TABLE podcast_trends_cache ADD COLUMN IF NOT EXISTS description TEXT")
                    cursor.execute("ALTER TABLE podcast_trends_cache ADD COLUMN IF NOT EXISTS url TEXT")
                    cursor.execute("ALTER TABLE podcast_trends_cache ADD COLUMN IF NOT EXISTS image_url TEXT")
                    cursor.execute("ALTER TABLE podcast_trends_cache ADD COLUMN IF NOT EXISTS language VARCHAR(10)")
                    cursor.execute("ALTER TABLE podcast_trends_cache ADD COLUMN IF NOT EXISTS country VARCHAR(100)")
                    # countryã‚«ãƒ©ãƒ ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯é•·ã•ã‚’æ‹¡å¼µ
                    try:
                        cursor.execute("ALTER TABLE podcast_trends_cache ALTER COLUMN country TYPE VARCHAR(100)")
                    except Exception:
                        pass  # æ—¢ã«VARCHAR(100)ã®å ´åˆã¯ã‚¨ãƒ©ãƒ¼ã‚’ç„¡è¦–
                    cursor.execute("ALTER TABLE podcast_trends_cache ADD COLUMN IF NOT EXISTS score INTEGER DEFAULT 0")
                    cursor.execute("ALTER TABLE podcast_trends_cache ADD COLUMN IF NOT EXISTS rank INTEGER DEFAULT 0")
                    cursor.execute("ALTER TABLE podcast_trends_cache ADD COLUMN IF NOT EXISTS trend_type VARCHAR(50)")
                    cursor.execute("ALTER TABLE podcast_trends_cache ADD COLUMN IF NOT EXISTS region VARCHAR(10)")
                conn.commit()
        except Exception as e:
            logger.warning(f"âš ï¸ podcast_trends_cacheã®ã‚¹ã‚­ãƒ¼ãƒæ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
        
        return self.save_to_cache(data, cache_key, region)
    
    def get_podcast_trends_from_cache(self, trend_type='best_podcasts', region='JP'):
        """Podcast Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—"""
        return self.get_from_cache('podcast_trends', region)
    
    def clear_podcast_trends_cache(self, trend_type='best_podcasts'):
        """Podcast Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        return self.clear_cache('podcast_trends', 'JP')
    
    def is_podcast_cache_valid(self, trend_type='best_podcasts', region='JP'):
        """Podcast Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ã‹ã©ã†ã‹ã‚’ç¢ºèª"""
        return self.is_cache_valid('podcast_trends', region, 24)
    
    # News Trends ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ã‚½ãƒƒãƒ‰
    def save_news_trends_to_cache(self, data, category='general', country='JP'):
        """News Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        if not data:
            return False
        
        # æ—¢å­˜ç’°å¢ƒã§ã¯news_trends_cacheã«ä¸è¶³ã‚«ãƒ©ãƒ ãŒã‚ã‚‹ã‚±ãƒ¼ã‚¹ãŒã‚ã‚‹ãŸã‚ã€ã“ã“ã§è£œå®Œã—ã¦ãŠã
        try:
            conn = self.get_connection()
            if conn:
                with conn.cursor() as cursor:
                    cursor.execute("ALTER TABLE news_trends_cache ADD COLUMN IF NOT EXISTS article_id VARCHAR(255)")
                    cursor.execute("ALTER TABLE news_trends_cache ADD COLUMN IF NOT EXISTS title TEXT")
                    cursor.execute("ALTER TABLE news_trends_cache ADD COLUMN IF NOT EXISTS source VARCHAR(255)")
                    cursor.execute("ALTER TABLE news_trends_cache ADD COLUMN IF NOT EXISTS published_at TIMESTAMP")
                    cursor.execute("ALTER TABLE news_trends_cache ADD COLUMN IF NOT EXISTS category VARCHAR(50)")
                    cursor.execute("ALTER TABLE news_trends_cache ADD COLUMN IF NOT EXISTS country VARCHAR(10)")
                conn.commit()
        except Exception as e:
            logger.warning(f"âš ï¸ news_trends_cacheã®ã‚¹ã‚­ãƒ¼ãƒæ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
        
        return self.save_to_cache(data, 'news_trends', country)
    
    def get_news_trends_from_cache(self, category='general', country='JP'):
        """News Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—"""
        return self.get_from_cache('news_trends', country)
    
    def clear_news_trends_cache(self, category='general', country='JP'):
        """News Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        return self.clear_cache('news_trends', country)
    
    def is_news_cache_valid(self, category='general', country='JP'):
        """News Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ã‹ã©ã†ã‹ã‚’ç¢ºèª"""
        return self.is_cache_valid('news_trends', country, 24)
    
    # World News Trends ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ã‚½ãƒƒãƒ‰
    def save_worldnews_trends_to_cache(self, data, cache_key='worldnews_trends', country='JP'):
        """World News Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        if not data:
            return False
        try:
            conn = self.get_connection()
            if conn:
                with conn.cursor() as cursor:
                    cursor.execute("ALTER TABLE worldnews_trends_cache ADD COLUMN IF NOT EXISTS url TEXT")
                    cursor.execute("ALTER TABLE worldnews_trends_cache ADD COLUMN IF NOT EXISTS description TEXT")
                    cursor.execute("ALTER TABLE worldnews_trends_cache ADD COLUMN IF NOT EXISTS image_url TEXT")
                conn.commit()
        except Exception as e:
            logger.warning(f"âš ï¸ worldnews_trends_cacheã®ã‚¹ã‚­ãƒ¼ãƒæ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
        return self.save_to_cache(data, cache_key, country)
    
    def get_worldnews_trends_from_cache(self, category='general', country='JP'):
        """World News Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—"""
        # æ¥ç¶šã‚’å–å¾—ï¼ˆæœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯ã¨å†æ¥ç¶šã‚’è‡ªå‹•ã§è¡Œã†ï¼‰
        try:
            conn = self.get_connection()
            if not conn:
                return None
        except Exception as e:
            logger.error(f"World News ã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return None
        
        try:
            with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                # World Newså°‚ç”¨ã®ã‚¯ã‚¨ãƒªï¼ˆcountryã‚«ãƒ©ãƒ ã§æ¤œç´¢ï¼‰
                cursor.execute("SELECT * FROM worldnews_trends_cache WHERE country = %s ORDER BY created_at DESC", (country.lower(),))
                data = cursor.fetchall()
                
                # RealDictCursorã®çµæœã‚’è¾æ›¸ã®ãƒªã‚¹ãƒˆã«å¤‰æ›
                result = []
                for row in data:
                    result.append(dict(row))
                
                return result
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ World News ã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return None
        except Exception as e:
            logger.error(f"World News ã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return None
    
    def clear_worldnews_trends_cache(self, category='general', country='JP'):
        """World News Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        return self.clear_cache('worldnews_trends', country)
    
    def is_worldnews_cache_valid(self, category='general', country='JP'):
        """World News Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ã‹ã©ã†ã‹ã‚’ç¢ºèª"""
        return self.is_cache_valid('worldnews_trends', country, 24)
    
    # Rakuten Trends ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ã‚½ãƒƒãƒ‰
    def save_rakuten_trends_to_cache(self, data, category='all'):
        """Rakuten Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        if not data:
            return False
        
        # æ—¢å­˜ç’°å¢ƒã§ã¯rakuten_trends_cacheã«ä¸è¶³ã‚«ãƒ©ãƒ ãŒã‚ã‚‹ã‚±ãƒ¼ã‚¹ãŒã‚ã‚‹ãŸã‚ã€ã“ã“ã§è£œå®Œã—ã¦ãŠã
        # ã‚¹ã‚­ãƒ¼ãƒæ›´æ–°ã¯åˆ¥ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ã§å®Ÿè¡Œï¼ˆã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ãƒ¡ã‚¤ãƒ³å‡¦ç†ã«å½±éŸ¿ã—ãªã„ã‚ˆã†ã«ï¼‰
        try:
            schema_conn = self.get_connection()
            if schema_conn:
                try:
                    with schema_conn.cursor() as cursor:
                        # INSERTæ–‡ã§ä½¿ç”¨ã—ã¦ã„ã‚‹ã‚«ãƒ©ãƒ ã‚’ç¢ºèªã—ã¦è¿½åŠ 
                        cursor.execute("ALTER TABLE rakuten_trends_cache ADD COLUMN IF NOT EXISTS genre_id VARCHAR(255)")
                        cursor.execute("ALTER TABLE rakuten_trends_cache ADD COLUMN IF NOT EXISTS review_count INTEGER DEFAULT 0")
                        cursor.execute("ALTER TABLE rakuten_trends_cache ADD COLUMN IF NOT EXISTS review_average FLOAT DEFAULT 0.0")
                        cursor.execute("ALTER TABLE rakuten_trends_cache ADD COLUMN IF NOT EXISTS image_url TEXT")
                        cursor.execute("ALTER TABLE rakuten_trends_cache ADD COLUMN IF NOT EXISTS url TEXT")
                        cursor.execute("ALTER TABLE rakuten_trends_cache ADD COLUMN IF NOT EXISTS shop_name VARCHAR(255)")
                        cursor.execute("ALTER TABLE rakuten_trends_cache ADD COLUMN IF NOT EXISTS sales_rank VARCHAR(255)")
                        cursor.execute("ALTER TABLE rakuten_trends_cache ADD COLUMN IF NOT EXISTS sales_count VARCHAR(255)")
                        cursor.execute("ALTER TABLE rakuten_trends_cache ADD COLUMN IF NOT EXISTS rank INTEGER DEFAULT 0")
                        cursor.execute("ALTER TABLE rakuten_trends_cache ADD COLUMN IF NOT EXISTS region VARCHAR(10) DEFAULT 'JP'")
                    schema_conn.commit()
                except Exception as schema_error:
                    logger.warning(f"âš ï¸ rakuten_trends_cacheã®ã‚¹ã‚­ãƒ¼ãƒæ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸ: {schema_error}", exc_info=True)
                    schema_conn.rollback()
                finally:
                    if schema_conn and not schema_conn.closed:
                        schema_conn.close()
        except Exception as e:
            logger.warning(f"âš ï¸ rakuten_trends_cacheã®ã‚¹ã‚­ãƒ¼ãƒæ›´æ–°ç”¨æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
        
        return self.save_to_cache(data, 'rakuten_trends', category)
    
    def get_rakuten_trends_from_cache(self, category='all'):
        """Rakuten Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—"""
        return self.get_from_cache('rakuten_trends', category)
    
    def clear_rakuten_trends_cache(self, category='all'):
        """Rakuten Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        return self.clear_cache('rakuten_trends', category)
    
    def is_rakuten_cache_valid(self, category='all'):
        """Rakuten Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ã‹ã©ã†ã‹ã‚’ç¢ºèª"""
        return self.is_cache_valid('rakuten_trends', category, 24)
    
    # Hatena Trends ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ã‚½ãƒƒãƒ‰
    def save_hatena_trends_to_cache(self, data, category='all'):
        """Hatena Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        return self.save_to_cache(data, 'hatena_trends', category)
    
    def get_hatena_trends_from_cache(self, category='all'):
        """Hatena Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—"""
        return self.get_from_cache('hatena_trends', category)
    
    def clear_hatena_trends_cache(self, category='all'):
        """Hatena Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        return self.clear_cache('hatena_trends', category)
    
    def is_hatena_cache_valid(self, category='all'):
        """Hatena Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ã‹ã©ã†ã‹ã‚’ç¢ºèª"""
        return self.is_cache_valid('hatena_trends', category, 24)
    
    # Twitch Trends ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ã‚½ãƒƒãƒ‰
    def save_twitch_trends_to_cache(self, data, trend_type='games'):
        """Twitch Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        return self.save_to_cache(data, 'twitch_trends', trend_type)
    
    def get_twitch_trends_from_cache(self, trend_type='games'):
        """Twitch Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—"""
        return self.get_from_cache('twitch_trends', trend_type)
    
    def clear_twitch_trends_cache(self, trend_type='games'):
        """Twitch Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        return self.clear_cache('twitch_trends', trend_type)
    
    def is_twitch_cache_valid(self, trend_type='games'):
        """Twitch Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ã‹ã©ã†ã‹ã‚’ç¢ºèª"""
        return self.is_cache_valid('twitch_trends', trend_type, 24)
    
    def get_cache_info(self, cache_key):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥æƒ…å ±ã‚’å–å¾—"""
        try:
            conn = self.get_connection()
            if not conn:
                return None
        except Exception as e:
            logger.error(f"âŒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return None
        
        try:
            with conn.cursor() as cursor:
                cursor.execute(
                    "SELECT last_updated, data_count FROM cache_status WHERE cache_key = %s",
                    (cache_key,)
                )
                result = cursor.fetchone()
                
                if result:
                    return {
                        'last_updated': result[0].isoformat() if result[0] else None,
                        'data_count': result[1]
                    }
                return None
                
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥æƒ…å ±å–å¾—ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return None
        except Exception as e:
            logger.error(f"âŒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return None
    
    def get_all_cache_status(self):
        """å…¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®çŠ¶æ…‹ã‚’å–å¾—"""
        try:
            conn = self.get_connection()
            if not conn:
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãŒå–å¾—ã§ããªã„å ´åˆã¯ç©ºã®çŠ¶æ…‹ã‚’è¿”ã™
                logger.warning("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãŒå–å¾—ã§ããªã„ãŸã‚ã€ç©ºã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥çŠ¶æ…‹ã‚’è¿”ã—ã¾ã™")
                return {}
            if not conn:
                return {}
        except Exception as e:
            logger.error(f"âŒ å…¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥çŠ¶æ…‹å–å¾—ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return {}
        
        try:
            with conn.cursor() as cursor:
                cursor.execute("SELECT cache_key, last_updated, data_count FROM cache_status")
                results = cursor.fetchall()
                
                status = {}
                for row in results:
                    status[row[0]] = {
                        'last_updated': row[1],
                        'data_count': row[2]
                    }
                return status
                
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ å…¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥çŠ¶æ…‹å–å¾—ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return {}
        except Exception as e:
            logger.error(f"âŒ å…¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥çŠ¶æ…‹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return {}
    
    def get_last_update_time(self):
        """æœ€å¾Œã®æ›´æ–°æ™‚åˆ»ã‚’å–å¾—"""
        try:
            conn = self.get_connection()
            if not conn:
                return None
        except Exception as e:
            logger.error(f"âŒ æœ€çµ‚æ›´æ–°æ™‚åˆ»å–å¾—ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return None
        
        try:
            with conn.cursor() as cursor:
                cursor.execute("SELECT MAX(last_updated) FROM cache_status")
                result = cursor.fetchone()
                return result[0] if result and result[0] else None
                
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ æœ€çµ‚æ›´æ–°æ™‚åˆ»å–å¾—ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return None
        except Exception as e:
            logger.error(f"âŒ æœ€çµ‚æ›´æ–°æ™‚åˆ»å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return None
    
    def clear_all_cache(self):
        """å…¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        try:
            conn = self.get_connection()
            if not conn:
                return False
        except Exception as e:
            logger.error(f"âŒ å…¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return False
        
        try:
            with conn.cursor() as cursor:
                # å…¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ã‚¯ãƒªã‚¢
                tables = [
                    'google_trends_cache',
                    'youtube_trends_cache',
                    'music_trends_cache',
                    'podcast_trends_cache',
                    'news_trends_cache',
                    'worldnews_trends_cache',
                    'rakuten_trends_cache',
                    'hatena_trends_cache',
                    'twitch_trends_cache'
                ]
                
                for table in tables:
                    cursor.execute(f"DELETE FROM {table}")
                
                # cache_statusã‚‚ã‚¯ãƒªã‚¢
                cursor.execute("DELETE FROM cache_status")
                
                conn.commit()
                logger.info("âœ… å…¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
                return True
                
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ å…¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return False
        except Exception as e:
            logger.error(f"âŒ å…¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return False
    
    def delete_old_cache_data(self, days=2):
        """å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ï¼ˆæŒ‡å®šæ—¥æ•°ä»¥ä¸ŠçµŒéã—ãŸãƒ‡ãƒ¼ã‚¿ï¼‰
        
        Args:
            days: å‰Šé™¤å¯¾è±¡ã¨ãªã‚‹æ—¥æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2æ—¥ï¼‰
        """
        try:
            conn = self.get_connection()
            if not conn:
                return False
        except Exception as e:
            logger.error(f"âŒ å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿å‰Šé™¤ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return False
        
        try:
            from datetime import timedelta
            cutoff_date = datetime.now() - timedelta(days=days)
            
            with conn.cursor() as cursor:
                deleted_counts = {}
                
                # å„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰å¤ã„ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤
                cache_tables = [
                    'google_trends_cache',
                    'youtube_trends_cache',
                    'music_trends_cache',
                    'podcast_trends_cache',
                    'news_trends_cache',
                    'worldnews_trends_cache',
                    'rakuten_trends_cache',
                    'hatena_trends_cache',
                    'twitch_trends_cache',
                    'reddit_trends_cache',
                    'hackernews_trends_cache',
                    'qiita_trends_cache',
                    'nhk_trends_cache',
                    'cnn_trends_cache',
                    'producthunt_trends_cache'
                ]
                
                for table in cache_tables:
                    try:
                        # ãƒ†ãƒ¼ãƒ–ãƒ«ã«å­˜åœ¨ã™ã‚‹æ—¥æ™‚ã‚«ãƒ©ãƒ ã‚’ç¢ºèªã—ã¦å‰Šé™¤
                        # ã¾ãšcreated_atã‚’è©¦ã—ã€ãªã‘ã‚Œã°cached_atã‚’ä½¿ç”¨
                        cursor.execute(f"""
                            SELECT column_name 
                            FROM information_schema.columns 
                            WHERE table_name = %s 
                            AND column_name IN ('created_at', 'cached_at')
                        """, (table,))
                        date_columns = [row[0] for row in cursor.fetchall()]
                        
                        if date_columns:
                            # å­˜åœ¨ã™ã‚‹ã‚«ãƒ©ãƒ ã§å‰Šé™¤
                            where_clause = ' OR '.join([f"{col} < %s" for col in date_columns])
                            params = [cutoff_date] * len(date_columns)
                            cursor.execute(f"DELETE FROM {table} WHERE {where_clause}", params)
                            count = cursor.rowcount
                            if count > 0:
                                deleted_counts[table] = count
                        else:
                            logger.debug(f"âš ï¸ {table}: æ—¥æ™‚ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                    except Exception as e:
                        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆãªã©ï¼‰
                        logger.debug(f"âš ï¸ {table}ã®å¤ã„ãƒ‡ãƒ¼ã‚¿å‰Šé™¤ã‚’ã‚¹ã‚­ãƒƒãƒ—: {e}")
                
                conn.commit()
                
                total_deleted = sum(deleted_counts.values())
                if total_deleted > 0:
                    logger.info(f"âœ… å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¾ã—ãŸ: åˆè¨ˆ{total_deleted}ä»¶")
                    for table, count in deleted_counts.items():
                        logger.info(f"   - {table}: {count}ä»¶")
                else:
                    logger.debug(f"ğŸ“Š å‰Šé™¤å¯¾è±¡ã®å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                
                return True
                
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿å‰Šé™¤ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return False
        except Exception as e:
            logger.error(f"âŒ å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return False
    
    def clear_cache_by_type(self, cache_type):
        """ç‰¹å®šã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        try:
            conn = self.get_connection()
            if not conn:
                return False
        except Exception as e:
            logger.error(f"âŒ {cache_type}ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return False
        
        try:
            with conn.cursor() as cursor:
                table_name = f"{cache_type}_cache"
                cursor.execute(f"DELETE FROM {table_name}")
                cursor.execute("DELETE FROM cache_status WHERE cache_key = %s", (cache_type,))
                conn.commit()
                logger.info(f"âœ… {cache_type}ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
                return True
                
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ {cache_type}ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return False
        except Exception as e:
            logger.error(f"âŒ {cache_type}ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return False
    
    # Reddit Trends ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ã‚½ãƒƒãƒ‰
    def save_reddit_trends_to_cache(self, data, subreddit='all'):
        """Reddit Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        if not data:
            return False
        
        try:
            conn = self.get_connection()
            if not conn:
                return False
        except Exception as e:
            logger.error(f"âŒ reddit_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return False
        
        try:
            with conn.cursor() as cursor:
                # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤
                cursor.execute("""
                    DELETE FROM reddit_trends_cache 
                    WHERE subreddit = %s
                """, (subreddit,))
                
                # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’æŒ¿å…¥
                for item in data:
                    cursor.execute("""
                        INSERT INTO reddit_trends_cache 
                        (post_id, title, url, subreddit, author, score, upvote_ratio, 
                         num_comments, permalink, is_video, domain, rank, region)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    """, (
                        item.get('post_id', ''),
                        item.get('title', ''),
                        item.get('url', ''),
                        subreddit,
                        item.get('author', ''),
                        item.get('score', 0),
                        item.get('upvote_ratio', 0.0),
                        item.get('num_comments', 0),
                        item.get('permalink', ''),
                        item.get('is_video', False),
                        item.get('domain', ''),
                        item.get('rank', 0),
                        subreddit
                    ))
                
                conn.commit()
                logger.info(f"âœ… reddit_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜ã—ã¾ã—ãŸ (subreddit: {subreddit}, {len(data)}ä»¶)")
                return True
                
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ reddit_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return False
        except Exception as e:
            logger.error(f"âŒ reddit_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            try:
                conn.rollback()
            except:
                pass
            return False
    
    def get_reddit_trends_from_cache(self, subreddit='all'):
        """Reddit Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—"""
        try:
            conn = self.get_connection()
            if not conn:
                return None
        except Exception as e:
            logger.error(f"âŒ Reddit Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return None
        
        try:
            with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                cursor.execute("""
                    SELECT post_id, title, url, subreddit, author, score, upvote_ratio,
                           num_comments, permalink, is_video, domain, rank, created_at
                    FROM reddit_trends_cache 
                    WHERE subreddit = %s 
                    ORDER BY rank
                """, (subreddit,))
                data = cursor.fetchall()
                
                # RealDictCursorã®çµæœã‚’è¾æ›¸ã®ãƒªã‚¹ãƒˆã«å¤‰æ›
                result = []
                for row in data:
                    result.append(dict(row))
                
                return result
                
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ Reddit Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return None
        except Exception as e:
            logger.error(f"âŒ Reddit Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return None
    
    def clear_reddit_trends_cache(self, subreddit='all'):
        """Reddit Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        try:
            conn = self.get_connection()
            if not conn:
                return False
        except Exception as e:
            logger.error(f"âŒ reddit_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return False
        
        try:
            with conn.cursor() as cursor:
                cursor.execute("""
                    DELETE FROM reddit_trends_cache 
                    WHERE subreddit = %s
                """, (subreddit,))
                conn.commit()
                logger.info(f"âœ… reddit_trendsã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ (subreddit: {subreddit})")
                return True
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ reddit_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return False
        except Exception as e:
            logger.error(f"âŒ reddit_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            try:
                conn.rollback()
            except:
                pass
            return False
    
    # Hacker News Trends ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ã‚½ãƒƒãƒ‰
    def save_hackernews_trends_to_cache(self, data, story_type='top'):
        """Hacker News Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        if not data:
            return False
        
        try:
            conn = self.get_connection()
            if not conn:
                return False
        except Exception as e:
            logger.error(f"âŒ hackernews_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return False
        
        try:
            with conn.cursor() as cursor:
                # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤
                cursor.execute("""
                    DELETE FROM hackernews_trends_cache 
                    WHERE story_type = %s
                """, (story_type,))
                
                # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’æŒ¿å…¥
                for item in data:
                    cursor.execute("""
                        INSERT INTO hackernews_trends_cache 
                        (story_id, title, url, score, author, story_time, comments, story_type, rank)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
                    """, (
                        item.get('story_id', 0),
                        item.get('title', ''),
                        item.get('url', ''),
                        item.get('score', 0),
                        item.get('by', ''),
                        item.get('time', 0),
                        item.get('comments', 0),
                        story_type,
                        item.get('rank', 0)
                    ))
                
                # cache_statusãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ›´æ–°
                from datetime import datetime
                now = datetime.now()
                cursor.execute("""
                    INSERT INTO cache_status (cache_key, last_updated, data_count)
                    VALUES (%s, %s, %s)
                    ON CONFLICT (cache_key) DO UPDATE SET
                        last_updated = EXCLUDED.last_updated,
                        data_count = EXCLUDED.data_count
                """, ('hackernews_trends', now, len(data)))
                
                conn.commit()
                logger.info(f"âœ… hackernews_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜ã—ã¾ã—ãŸ (type: {story_type}, {len(data)}ä»¶)")
                return True
                
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ hackernews_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return False
        except Exception as e:
            logger.error(f"âŒ hackernews_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            try:
                conn.rollback()
            except:
                pass
            return False
    
    def get_hackernews_trends_from_cache(self, story_type='top'):
        """Hacker News Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—"""
        try:
            conn = self.get_connection()
            if not conn:
                return None
        except Exception as e:
            logger.error(f"âŒ Hacker News Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return None
        
        try:
            with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                cursor.execute("""
                    SELECT story_id, title, url, score, author, story_time, 
                           comments, story_type, rank, created_at
                    FROM hackernews_trends_cache 
                    WHERE story_type = %s 
                    ORDER BY rank
                """, (story_type,))
                data = cursor.fetchall()
                
                # RealDictCursorã®çµæœã‚’è¾æ›¸ã®ãƒªã‚¹ãƒˆã«å¤‰æ›
                result = []
                for row in data:
                    result.append(dict(row))
                
                return result
                
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ Hacker News Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return None
        except Exception as e:
            logger.error(f"âŒ Hacker News Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return None
    
    def clear_hackernews_trends_cache(self, story_type='top'):
        """Hacker News Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        try:
            conn = self.get_connection()
            if not conn:
                return False
        except Exception as e:
            logger.error(f"âŒ hackernews_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return False
        
        try:
            with conn.cursor() as cursor:
                cursor.execute("""
                    DELETE FROM hackernews_trends_cache 
                    WHERE story_type = %s
                """, (story_type,))
                conn.commit()
                logger.info(f"âœ… hackernews_trendsã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ (type: {story_type})")
                return True
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ hackernews_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return False
        except Exception as e:
            logger.error(f"âŒ hackernews_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            try:
                conn.rollback()
            except:
                pass
            return False
    
    # Qiita Trends ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ã‚½ãƒƒãƒ‰
    def save_qiita_trends_to_cache(self, data):
        """Qiita Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        if not data:
            return False
        
        try:
            conn = self.get_connection()
            if not conn:
                return False
        except Exception as e:
            logger.error(f"âŒ qiita_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return False
        
        try:
            with conn.cursor() as cursor:
                # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤
                cursor.execute("""
                    DELETE FROM qiita_trends_cache
                """)
                
                # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’æŒ¿å…¥
                for item in data:
                    # ã‚¿ã‚°ã‚’JSONæ–‡å­—åˆ—ã«å¤‰æ›
                    tags_json = json.dumps(item.get('tags', []), ensure_ascii=False)
                    
                    cursor.execute("""
                        INSERT INTO qiita_trends_cache 
                        (item_id, title, url, user_id, user_name, likes_count, stocks_count,
                         comments_count, created_at, updated_at, tags, rank)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    """, (
                        item.get('item_id', ''),
                        item.get('title', ''),
                        item.get('url', ''),
                        item.get('user_id', ''),
                        item.get('user_name', ''),
                        item.get('likes_count', 0),
                        item.get('stocks_count', 0),
                        item.get('comments_count', 0),
                        item.get('created_at', ''),
                        item.get('updated_at', ''),
                        tags_json,
                        item.get('rank', 0)
                    ))
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ›´æ–°
                cursor.execute(
                    "INSERT INTO cache_status (cache_key, last_updated, data_count) VALUES (%s, %s, %s) ON CONFLICT (cache_key) DO UPDATE SET last_updated = %s, data_count = %s",
                    ('qiita_trends', datetime.now(), len(data), datetime.now(), len(data))
                )
                
                conn.commit()
                logger.info(f"âœ… qiita_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜ã—ã¾ã—ãŸ ({len(data)}ä»¶)")
                return True
                
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ qiita_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return False
        except Exception as e:
            logger.error(f"âŒ qiita_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            try:
                conn.rollback()
            except:
                pass
            return False
    
    def get_qiita_trends_from_cache(self):
        """Qiita Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—"""
        try:
            conn = self.get_connection()
            if not conn:
                return None
        except Exception as e:
            logger.error(f"âŒ Qiita Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return None
        
        try:
            with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                cursor.execute("""
                    SELECT item_id, title, url, user_id, user_name, likes_count, stocks_count,
                           comments_count, created_at, updated_at, tags, rank, cached_at
                    FROM qiita_trends_cache 
                    ORDER BY rank
                """)
                data = cursor.fetchall()
                
                # RealDictCursorã®çµæœã‚’è¾æ›¸ã®ãƒªã‚¹ãƒˆã«å¤‰æ›
                result = []
                for row in data:
                    item = dict(row)
                    # ã‚¿ã‚°ã‚’JSONã‹ã‚‰ãƒªã‚¹ãƒˆã«å¤‰æ›
                    if item.get('tags'):
                        try:
                            item['tags'] = json.loads(item['tags'])
                        except:
                            item['tags'] = []
                    result.append(item)
                
                return result
                
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ Qiita Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return None
        except Exception as e:
            logger.error(f"âŒ Qiita Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return None
    
    def clear_qiita_trends_cache(self):
        """Qiita Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        try:
            conn = self.get_connection()
            if not conn:
                return False
        except Exception as e:
            logger.error(f"âŒ qiita_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return False
        
        try:
            with conn.cursor() as cursor:
                cursor.execute("""
                    DELETE FROM qiita_trends_cache
                """)
                conn.commit()
                logger.info(f"âœ… qiita_trendsã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
                return True
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ qiita_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return False
        except Exception as e:
            logger.error(f"âŒ qiita_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            try:
                conn.rollback()
            except:
                pass
            return False
    
    # NHK Trends ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ã‚½ãƒƒãƒ‰
    def save_nhk_trends_to_cache(self, data):
        """NHK Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        if not data:
            return False
        try:
            conn = self.get_connection()
            if not conn:
                return False
        except Exception as e:
            logger.error(f"âŒ nhk_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return False
        try:
            with conn.cursor() as cursor:
                cursor.execute("DELETE FROM nhk_trends_cache")
                for item in data:
                    cursor.execute("""
                        INSERT INTO nhk_trends_cache 
                        (title, url, published_date, description, rank)
                        VALUES (%s, %s, %s, %s, %s)
                    """, (
                        item.get('title', ''),
                        item.get('url', ''),
                        item.get('published_date'),
                        item.get('description', ''),
                        item.get('rank', 0)
                    ))
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ›´æ–°
                cursor.execute(
                    "INSERT INTO cache_status (cache_key, last_updated, data_count) VALUES (%s, %s, %s) ON CONFLICT (cache_key) DO UPDATE SET last_updated = %s, data_count = %s",
                    ('nhk_trends', datetime.now(), len(data), datetime.now(), len(data))
                )
                
                conn.commit()
                logger.info(f"âœ… nhk_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜ã—ã¾ã—ãŸ ({len(data)}ä»¶)")
                return True
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ nhk_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return False
        except Exception as e:
            logger.error(f"âŒ nhk_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            try:
                conn.rollback()
            except:
                pass
            return False

    def get_nhk_trends_from_cache(self):
        """NHK Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—"""
        try:
            conn = self.get_connection()
            if not conn:
                return None
        except Exception as e:
            logger.error(f"âŒ NHK Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return None
        try:
            with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                cursor.execute("""
                    SELECT title, url, published_date, description, rank, cached_at
                    FROM nhk_trends_cache 
                    ORDER BY rank
                """)
                data = cursor.fetchall()
                result = []
                for row in data:
                    row_dict = dict(row)
                    # published_dateã‚’ISOå½¢å¼ã®æ–‡å­—åˆ—ã«å¤‰æ›
                    if row_dict.get('published_date'):
                        if isinstance(row_dict['published_date'], datetime):
                            row_dict['published_date'] = row_dict['published_date'].isoformat()
                    result.append(row_dict)
                return result
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ NHK Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return None
        except Exception as e:
            logger.error(f"âŒ NHK Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return None

    def clear_nhk_trends_cache(self):
        """NHK Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        try:
            conn = self.get_connection()
            if not conn:
                return False
        except Exception as e:
            logger.error(f"âŒ nhk_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return False
        try:
            with conn.cursor() as cursor:
                cursor.execute("DELETE FROM nhk_trends_cache")
                conn.commit()
                logger.info(f"âœ… nhk_trendsã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
                return True
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ nhk_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return False
        except Exception as e:
            logger.error(f"âŒ nhk_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            try:
                conn.rollback()
            except:
                pass
            return False
    
    # CNN Trends ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ã‚½ãƒƒãƒ‰
    def save_cnn_trends_to_cache(self, data):
        """CNN Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        if not data:
            return False
        try:
            conn = self.get_connection()
            if not conn:
                return False
        except Exception as e:
            logger.error(f"âŒ cnn_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return False
        try:
            with conn.cursor() as cursor:
                cursor.execute("DELETE FROM cnn_trends_cache")
                for item in data:
                    cursor.execute("""
                        INSERT INTO cnn_trends_cache 
                        (title, url, published_date, description, rank)
                        VALUES (%s, %s, %s, %s, %s)
                    """, (
                        item.get('title', ''),
                        item.get('url', ''),
                        item.get('published_date'),
                        item.get('description', ''),
                        item.get('rank', 0)
                    ))
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ›´æ–°
                cursor.execute(
                    "INSERT INTO cache_status (cache_key, last_updated, data_count) VALUES (%s, %s, %s) ON CONFLICT (cache_key) DO UPDATE SET last_updated = %s, data_count = %s",
                    ('cnn_trends', datetime.now(), len(data), datetime.now(), len(data))
                )
                
                conn.commit()
                logger.info(f"âœ… cnn_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜ã—ã¾ã—ãŸ ({len(data)}ä»¶)")
                return True
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ cnn_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return False
        except Exception as e:
            logger.error(f"âŒ cnn_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            try:
                conn.rollback()
            except:
                pass
            return False

    def get_cnn_trends_from_cache(self):
        """CNN Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—"""
        try:
            conn = self.get_connection()
            if not conn:
                return None
        except Exception as e:
            logger.error(f"âŒ CNN Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return None
        try:
            with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                cursor.execute("""
                    SELECT title, url, published_date, description, rank, cached_at
                    FROM cnn_trends_cache 
                    ORDER BY rank
                """)
                data = cursor.fetchall()
                result = []
                for row in data:
                    row_dict = dict(row)
                    # published_dateã‚’ISOå½¢å¼ã®æ–‡å­—åˆ—ã«å¤‰æ›
                    if row_dict.get('published_date'):
                        if isinstance(row_dict['published_date'], datetime):
                            row_dict['published_date'] = row_dict['published_date'].isoformat()
                    result.append(row_dict)
                return result
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ CNN Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return None
        except Exception as e:
            logger.error(f"âŒ CNN Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return None

    def clear_cnn_trends_cache(self):
        """CNN Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        try:
            conn = self.get_connection()
            if not conn:
                return False
        except Exception as e:
            logger.error(f"âŒ cnn_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return False
        try:
            with conn.cursor() as cursor:
                cursor.execute("DELETE FROM cnn_trends_cache")
                conn.commit()
                logger.info(f"âœ… cnn_trendsã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
                return True
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ cnn_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return False
        except Exception as e:
            logger.error(f"âŒ cnn_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            try:
                conn.rollback()
            except:
                pass
            return False
    
    # Product Hunt Trends ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ã‚½ãƒƒãƒ‰
    def save_producthunt_trends_to_cache(self, data):
        """Product Hunt Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        if not data:
            return False
        
        try:
            conn = self.get_connection()
            if not conn:
                return False
        except Exception as e:
            logger.error(f"âŒ producthunt_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return False
        
        try:
            with conn.cursor() as cursor:
                # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤
                cursor.execute("""
                    DELETE FROM producthunt_trends_cache
                """)
                
                # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’æŒ¿å…¥
                for item in data:
                    # ãƒˆãƒ”ãƒƒã‚¯ã‚’JSONæ–‡å­—åˆ—ã«å¤‰æ›
                    topics_json = json.dumps(item.get('topics', []), ensure_ascii=False)
                    
                    cursor.execute("""
                        INSERT INTO producthunt_trends_cache 
                        (product_id, name, tagline, description, url, website, votes_count, comments_count,
                         created_at, topics, user_name, user_username, rank)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    """, (
                        item.get('product_id', ''),
                        item.get('name', ''),
                        item.get('tagline', ''),
                        item.get('description', ''),
                        item.get('url', ''),
                        item.get('website', ''),
                        item.get('votes_count', 0),
                        item.get('comments_count', 0),
                        item.get('created_at', ''),
                        topics_json,
                        item.get('user_name', ''),
                        item.get('user_username', ''),
                        item.get('rank', 0)
                    ))
                
                # cache_statusãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ›´æ–°
                from datetime import datetime
                now = datetime.now()
                cursor.execute("""
                    INSERT INTO cache_status (cache_key, last_updated, data_count)
                    VALUES (%s, %s, %s)
                    ON CONFLICT (cache_key) DO UPDATE SET
                        last_updated = EXCLUDED.last_updated,
                        data_count = EXCLUDED.data_count
                """, ('producthunt_trends', now, len(data)))
                
                conn.commit()
                logger.info(f"âœ… producthunt_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜ã—ã¾ã—ãŸ ({len(data)}ä»¶)")
                return True
                
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ producthunt_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return False
        except Exception as e:
            logger.error(f"âŒ producthunt_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            try:
                conn.rollback()
            except:
                pass
            return False
    
    def get_producthunt_trends_from_cache(self):
        """Product Hunt Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—"""
        try:
            conn = self.get_connection()
            if not conn:
                return None
        except Exception as e:
            logger.error(f"âŒ Product Hunt Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return None
        
        try:
            with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                cursor.execute("""
                    SELECT product_id, name, tagline, description, url, website, votes_count, comments_count,
                           created_at, topics, user_name, user_username, rank, cached_at
                    FROM producthunt_trends_cache 
                    ORDER BY rank
                """)
                data = cursor.fetchall()
                
                # RealDictCursorã®çµæœã‚’è¾æ›¸ã®ãƒªã‚¹ãƒˆã«å¤‰æ›
                result = []
                for row in data:
                    item = dict(row)
                    # ãƒˆãƒ”ãƒƒã‚¯ã‚’JSONã‹ã‚‰ãƒªã‚¹ãƒˆã«å¤‰æ›
                    if item.get('topics'):
                        try:
                            item['topics'] = json.loads(item['topics'])
                        except:
                            item['topics'] = []
                    result.append(item)
                
                return result
                
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ Product Hunt Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return None
        except Exception as e:
            logger.error(f"âŒ Product Hunt Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return None
    
    def clear_producthunt_trends_cache(self):
        """Product Hunt Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        try:
            conn = self.get_connection()
            if not conn:
                return False
        except Exception as e:
            logger.error(f"âŒ producthunt_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return False
        
        try:
            with conn.cursor() as cursor:
                cursor.execute("""
                    DELETE FROM producthunt_trends_cache
                """)
                conn.commit()
                logger.info(f"âœ… producthunt_trendsã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
                return True
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ producthunt_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return False
        except Exception as e:
            logger.error(f"âŒ producthunt_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            try:
                conn.rollback()
            except:
                pass
            return False
    
    # Stock Trends ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ã‚½ãƒƒãƒ‰
    def save_stock_trends_to_cache(self, data, market='US'):
        """Stock Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        if not data:
            return False
        
        try:
            conn = self.get_connection()
            if not conn:
                return False
        except Exception as e:
            logger.error(f"âŒ stock_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return False
        
        try:
            with conn.cursor() as cursor:
                # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤
                cursor.execute("""
                    DELETE FROM stock_trends_cache 
                    WHERE market = %s
                """, (market,))
                
                # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’æŒ¿å…¥
                for item in data:
                    cursor.execute("""
                        INSERT INTO stock_trends_cache
                        (symbol, name, current_price, previous_price, change, change_percent,
                         volume, market_cap, market, rank, updated_at)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    """, (
                        item.get('symbol', ''),
                        item.get('name', ''),
                        item.get('current_price', 0),
                        item.get('previous_price', 0),
                        item.get('change', 0),
                        item.get('change_percent', 0),
                        item.get('volume', 0),
                        item.get('market_cap', 0),
                        market,
                        item.get('rank', 0),
                        item.get('updated_at')
                    ))
                # cache_statusãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ›´æ–°
                from datetime import datetime
                now = datetime.now()
                cursor.execute("""
                    INSERT INTO cache_status (cache_key, last_updated, data_count)
                    VALUES (%s, %s, %s)
                    ON CONFLICT (cache_key) DO UPDATE SET
                        last_updated = EXCLUDED.last_updated,
                        data_count = EXCLUDED.data_count
                """, ('stock_trends', now, len(data)))
                
                conn.commit()
                logger.info(f"âœ… stock_trendsã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜ã—ã¾ã—ãŸ (market: {market}, {len(data)}ä»¶)")
                return True
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ stock_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return False
        except Exception as e:
            logger.error(f"âŒ stock_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            try:
                conn.rollback()
            except:
                pass
            return False
    
    def get_stock_trends_from_cache(self, market='US'):
        """Stock Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—"""
        try:
            conn = self.get_connection()
            if not conn:
                return None
        except Exception as e:
            logger.error(f"âŒ Stock Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return None
        
        try:
            with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                cursor.execute("""
                    SELECT symbol, name, current_price, previous_price, change, change_percent,
                           volume, market_cap, market, rank, updated_at, cached_at
                    FROM stock_trends_cache 
                    WHERE market = %s 
                    ORDER BY rank
                """, (market,))
                data = cursor.fetchall()
                
                # RealDictCursorã®çµæœã‚’è¾æ›¸ã®ãƒªã‚¹ãƒˆã«å¤‰æ›
                result = []
                for row in data:
                    result.append(dict(row))
                
                return result
                
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ Stock Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return None
        except Exception as e:
            logger.error(f"âŒ Stock Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return None
    
    def clear_stock_trends_cache(self, market='US'):
        """Stock Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        try:
            conn = self.get_connection()
            if not conn:
                return False
        except Exception as e:
            logger.error(f"âŒ stock_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return False
        
        try:
            with conn.cursor() as cursor:
                cursor.execute("""
                    DELETE FROM stock_trends_cache 
                    WHERE market = %s
                """, (market,))
                conn.commit()
                logger.info(f"âœ… stock_trendsã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ (market: {market})")
                return True
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ stock_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return False
        except Exception as e:
            logger.error(f"âŒ stock_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            try:
                conn.rollback()
            except:
                pass
            return False
    
    # Crypto Trends ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ã‚½ãƒƒãƒ‰
    def save_crypto_trends_to_cache(self, data):
        """Crypto Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        if not data:
            return False
        
        try:
            conn = self.get_connection()
            if not conn:
                return False
        except Exception as e:
            logger.error(f"âŒ crypto_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return False
        
        try:
            with conn.cursor() as cursor:
                # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤
                cursor.execute("DELETE FROM crypto_trends_cache")
                
                # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’æŒ¿å…¥
                for item in data:
                    cursor.execute("""
                        INSERT INTO crypto_trends_cache
                        (coin_id, symbol, name, market_cap_rank, search_score,
                         current_price, price_change_24h, price_change_percentage_24h,
                         market_cap, volume_24h, image_url, rank, updated_at)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    """, (
                        item.get('coin_id', ''),
                        item.get('symbol', ''),
                        item.get('name', ''),
                        item.get('market_cap_rank', 0),
                        item.get('search_score', 0),
                        item.get('current_price', 0),
                        item.get('price_change_24h', 0),
                        item.get('price_change_percentage_24h', 0),
                        item.get('market_cap', 0),
                        item.get('volume_24h', 0),
                        item.get('image_url', ''),
                        item.get('rank', 0),
                        item.get('updated_at')
                    ))
                # cache_statusãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ›´æ–°
                from datetime import datetime
                now = datetime.now()
                cursor.execute("""
                    INSERT INTO cache_status (cache_key, last_updated, data_count)
                    VALUES (%s, %s, %s)
                    ON CONFLICT (cache_key) DO UPDATE SET
                        last_updated = EXCLUDED.last_updated,
                        data_count = EXCLUDED.data_count
                """, ('crypto_trends', now, len(data)))
                
                conn.commit()
                logger.info(f"âœ… crypto_trendsã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜ã—ã¾ã—ãŸ ({len(data)}ä»¶)")
                return True
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ crypto_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return False
        except Exception as e:
            logger.error(f"âŒ crypto_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            try:
                conn.rollback()
            except:
                pass
            return False
    
    def get_crypto_trends_from_cache(self):
        """Crypto Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—"""
        try:
            conn = self.get_connection()
            if not conn:
                return None
        except Exception as e:
            logger.error(f"âŒ Crypto Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return None
        
        try:
            with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                cursor.execute("""
                    SELECT coin_id, symbol, name, market_cap_rank, search_score,
                           current_price, price_change_24h, price_change_percentage_24h,
                           market_cap, volume_24h, image_url, rank, updated_at, cached_at
                    FROM crypto_trends_cache 
                    ORDER BY rank
                """)
                data = cursor.fetchall()
                
                # RealDictCursorã®çµæœã‚’è¾æ›¸ã®ãƒªã‚¹ãƒˆã«å¤‰æ›
                result = []
                for row in data:
                    result.append(dict(row))
                
                return result
                
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ Crypto Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return None
        except Exception as e:
            logger.error(f"âŒ Crypto Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return None
    
    def clear_crypto_trends_cache(self):
        """Crypto Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        try:
            conn = self.get_connection()
            if not conn:
                return False
        except Exception as e:
            logger.error(f"âŒ crypto_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return False
        
        try:
            with conn.cursor() as cursor:
                cursor.execute("DELETE FROM crypto_trends_cache")
                conn.commit()
                logger.info(f"âœ… crypto_trendsã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
                return True
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ crypto_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return False
        except Exception as e:
            logger.error(f"âŒ crypto_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            try:
                conn.rollback()
            except:
                pass
            return False
    
    def save_movie_trends_to_cache(self, data, country='JP'):
        """Movie Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        if not data:
            return False
        
        try:
            conn = self.get_connection()
            if not conn:
                return False
        except Exception as e:
            logger.error(f"âŒ movie_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return False
        
        try:
            with conn.cursor() as cursor:
                # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ï¼ˆå›½åˆ¥ï¼‰
                cursor.execute("DELETE FROM movie_trends_cache WHERE country = %s", (country,))
                
                # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’æŒ¿å…¥
                for item in data:
                    cursor.execute("""
                        INSERT INTO movie_trends_cache
                        (country, movie_id, title, original_title, overview, popularity,
                         vote_average, vote_count, release_date, poster_path,
                         backdrop_path, poster_url, backdrop_url, rank, updated_at)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    """, (
                        country,
                        item.get('id', 0),
                        item.get('title', ''),
                        item.get('original_title', ''),
                        item.get('overview', ''),
                        item.get('popularity', 0),
                        item.get('vote_average', 0),
                        item.get('vote_count', 0),
                        item.get('release_date', ''),
                        item.get('poster_path', ''),
                        item.get('backdrop_path', ''),
                        item.get('poster_url', ''),
                        item.get('backdrop_url', ''),
                        item.get('rank', 0),
                        item.get('updated_at')
                    ))
                # cache_statusãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ›´æ–°
                from datetime import datetime
                now = datetime.now()
                cache_key = f'movie_trends_{country}'
                cursor.execute("""
                    INSERT INTO cache_status (cache_key, last_updated, data_count)
                    VALUES (%s, %s, %s)
                    ON CONFLICT (cache_key) DO UPDATE SET
                        last_updated = EXCLUDED.last_updated,
                        data_count = EXCLUDED.data_count
                """, (cache_key, now, len(data)))
                
                conn.commit()
                logger.info(f"âœ… movie_trendsã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜ã—ã¾ã—ãŸ (country: {country}, {len(data)}ä»¶)")
                return True
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ movie_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return False
        except Exception as e:
            logger.error(f"âŒ movie_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            try:
                conn.rollback()
            except:
                pass
            return False
    
    def get_movie_trends_from_cache(self, country='JP'):
        """Movie Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—"""
        try:
            conn = self.get_connection()
            if not conn:
                return None
        except Exception as e:
            logger.error(f"âŒ Movie Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return None
        
        try:
            with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                cursor.execute("""
                    SELECT country, movie_id, title, original_title, overview, popularity,
                           vote_average, vote_count, release_date, poster_path,
                           backdrop_path, poster_url, backdrop_url, rank, updated_at, cached_at
                    FROM movie_trends_cache 
                    WHERE country = %s
                    ORDER BY rank
                """, (country,))
                data = cursor.fetchall()
                
                # RealDictCursorã®çµæœã‚’è¾æ›¸ã®ãƒªã‚¹ãƒˆã«å¤‰æ›
                result = []
                for row in data:
                    result.append(dict(row))
                
                return result
                
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ Movie Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return None
        except Exception as e:
            logger.error(f"âŒ Movie Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return None
    
    def clear_movie_trends_cache(self, country='JP'):
        """Movie Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        try:
            conn = self.get_connection()
            if not conn:
                return False
        except Exception as e:
            logger.error(f"âŒ movie_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return False
        
        try:
            with conn.cursor() as cursor:
                cursor.execute("DELETE FROM movie_trends_cache WHERE country = %s", (country,))
                conn.commit()
                logger.info(f"âœ… movie_trendsã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
                return True
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ movie_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return False
        except Exception as e:
            logger.error(f"âŒ movie_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            try:
                conn.rollback()
            except:
                pass
            return False
    
    def save_book_trends_to_cache(self, data, country='JP'):
        """Book Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        if not data:
            return False
        
        try:
            conn = self.get_connection()
            if not conn:
                return False
        except Exception as e:
            logger.error(f"âŒ book_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return False
        
        try:
            with conn.cursor() as cursor:
                # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ï¼ˆå›½åˆ¥ï¼‰
                cursor.execute("DELETE FROM book_trends_cache WHERE country = %s", (country,))
                
                # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’æŒ¿å…¥
                for item in data:
                    # é…åˆ—ã‚„è¾æ›¸ã‚’JSONæ–‡å­—åˆ—ã«å¤‰æ›
                    authors = item.get('authors', [])
                    authors_str = json.dumps(authors, ensure_ascii=False) if isinstance(authors, list) else str(authors)
                    categories = item.get('categories', [])
                    categories_str = json.dumps(categories, ensure_ascii=False) if isinstance(categories, list) else str(categories)
                    
                    # updated_atã®å‡¦ç†
                    updated_at = item.get('updated_at')
                    if not updated_at:
                        from datetime import timezone
                        updated_at = datetime.now(timezone.utc)
                    
                    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æº–å‚™
                    params = (
                        country,
                        item.get('id', ''),
                        item.get('isbn', ''),
                        item.get('title', ''),
                        item.get('subtitle', ''),
                        item.get('author', ''),
                        authors_str,
                        item.get('publisher', ''),
                        item.get('price', 0),
                        item.get('sales', 0),
                        item.get('published_date', ''),
                        item.get('release_date', ''),
                        item.get('description', ''),
                        item.get('page_count', 0),
                        categories_str,
                        item.get('average_rating', 0),
                        item.get('ratings_count', 0),
                        item.get('language', ''),
                        item.get('item_url', ''),
                        item.get('affiliate_url', ''),
                        item.get('preview_link', ''),
                        item.get('info_link', ''),
                        item.get('buy_link', ''),
                        item.get('image_url', ''),
                        item.get('thumbnail', ''),
                        item.get('small_thumbnail', ''),
                        item.get('medium', ''),
                        item.get('large', ''),
                        item.get('rank', 0),
                        updated_at
                    )
                    
                    cursor.execute("""
                        INSERT INTO book_trends_cache
                        (country, book_id, isbn, title, subtitle, author, authors, publisher,
                         price, sales, published_date, release_date, description, page_count,
                         categories, average_rating, ratings_count, language, item_url,
                         affiliate_url, preview_link, info_link, buy_link, image_url,
                         thumbnail, small_thumbnail, medium, large, rank, updated_at)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    """, params)
                # cache_statusãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ›´æ–°
                from datetime import datetime
                now = datetime.now()
                cursor.execute("""
                    INSERT INTO cache_status (cache_key, last_updated, data_count)
                    VALUES (%s, %s, %s)
                    ON CONFLICT (cache_key) DO UPDATE SET
                        last_updated = EXCLUDED.last_updated,
                        data_count = EXCLUDED.data_count
                """, (f'book_trends_{country}', now, len(data)))
                
                conn.commit()
                logger.info(f"âœ… book_trendsã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜ã—ã¾ã—ãŸ ({country}, {len(data)}ä»¶)")
                return True
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ book_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return False
        except Exception as e:
            logger.error(f"âŒ book_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            try:
                conn.rollback()
            except:
                pass
            return False
    
    def get_book_trends_from_cache(self, country='JP'):
        """Book Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—"""
        try:
            conn = self.get_connection()
            if not conn:
                return None
        except Exception as e:
            logger.error(f"âŒ Book Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return None
        
        try:
            with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                cursor.execute("""
                    SELECT country, book_id, isbn, title, subtitle, author, authors, publisher,
                           price, sales, published_date, release_date, description, page_count,
                           categories, average_rating, ratings_count, language, item_url,
                           affiliate_url, preview_link, info_link, buy_link, image_url,
                           thumbnail, small_thumbnail, medium, large, rank, updated_at, cached_at
                    FROM book_trends_cache 
                    WHERE country = %s
                    ORDER BY rank
                """, (country,))
                data = cursor.fetchall()
                
                # RealDictCursorã®çµæœã‚’è¾æ›¸ã®ãƒªã‚¹ãƒˆã«å¤‰æ›
                result = []
                for row in data:
                    row_dict = dict(row)
                    # JSONæ–‡å­—åˆ—ã‚’é…åˆ—ã«å¤‰æ›
                    if row_dict.get('authors'):
                        try:
                            row_dict['authors'] = json.loads(row_dict['authors']) if isinstance(row_dict['authors'], str) else row_dict['authors']
                        except:
                            row_dict['authors'] = []
                    if row_dict.get('categories'):
                        try:
                            row_dict['categories'] = json.loads(row_dict['categories']) if isinstance(row_dict['categories'], str) else row_dict['categories']
                        except:
                            row_dict['categories'] = []
                    result.append(row_dict)
                
                return result
                
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ Book Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return None
        except Exception as e:
            logger.error(f"âŒ Book Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return None
    
    def clear_book_trends_cache(self, country='JP'):
        """Book Trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        try:
            conn = self.get_connection()
            if not conn:
                return False
        except Exception as e:
            logger.error(f"âŒ book_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            return False
        
        try:
            with conn.cursor() as cursor:
                cursor.execute("DELETE FROM book_trends_cache WHERE country = %s", (country,))
                conn.commit()
                logger.info(f"âœ… book_trendsã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ ({country})")
                return True
        except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
            logger.warning(f"âš ï¸ book_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.connection = None
            return False
        except Exception as e:
            logger.error(f"âŒ book_trendsã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            try:
                conn.rollback()
            except:
                pass
            return False
    
    def close(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’é–‰ã˜ã‚‹"""
        if self.connection:
            self.connection.close()
            logger.info("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’é–‰ã˜ã¾ã—ãŸ")
