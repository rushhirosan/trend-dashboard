import os
import re
import requests
from datetime import datetime
from database_config import TrendsCache
from utils.logger_config import get_logger
from utils.rate_limiter import get_rate_limiter

logger = get_logger(__name__)

class CNNTrendsManager:
    """CNNãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’å–å¾—ãƒ»ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹ï¼ˆNewsAPIã‚’ä½¿ç”¨ï¼‰"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        # NewsAPIã‚­ãƒ¼ã‚’å–å¾—ï¼ˆCNN RSSãƒ•ã‚£ãƒ¼ãƒ‰ãŒ2024å¹´8æœˆã§æ­¢ã¾ã£ã¦ã„ã‚‹ãŸã‚ã€NewsAPIã‚’ä½¿ç”¨ï¼‰
        self.news_api_key = os.getenv('NEWS_API_KEY')
        self.news_api_base_url = "https://newsapi.org/v2"
        self.db = TrendsCache()
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™: News APIï¼ˆCNNç”¨ï¼‰ã¯100 requests/æ—¥ï¼ˆä¿å®ˆçš„ã«10ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/åˆ†ã«è¨­å®šï¼‰
        self.rate_limiter = get_rate_limiter('cnn', max_requests=10, window_seconds=60)
        
        if not self.news_api_key:
            logger.warning("âš ï¸ NEWS_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚CNNè¨˜äº‹ã¯å–å¾—ã§ãã¾ã›ã‚“")
        else:
            logger.info("âœ… CNN Trends ManageråˆæœŸåŒ–: NewsAPIã‚’ä½¿ç”¨ã—ã¦æœ€æ–°ã®CNNè¨˜äº‹ã‚’å–å¾—ã—ã¾ã™")
    
    def get_trends(self, limit=25, force_refresh=False):
        """CNNãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ãªã„å ´åˆã®ã¿NewsAPIã‚’å‘¼ã³å‡ºã—ï¼‰"""
        try:
            if force_refresh:
                logger.info(f"ğŸ”„ CNN force_refresh: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã™")
                self.db.clear_cnn_trends_cache()
            
            cached_data = self.db.get_cnn_trends_from_cache()
            
            if cached_data:
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã«ã‚‚é‡è¤‡æ’é™¤ã‚’é©ç”¨
                cached_data = self._remove_duplicates(cached_data)
                
                # å¤ã„ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆ2024å¹´ä»¥é™ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰
                # æ³¨: CNN RSSãƒ•ã‚£ãƒ¼ãƒ‰ãŒ2023å¹´ã®å¤ã„ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™ã“ã¨ãŒã‚ã‚‹ãŸã‚ã€2024å¹´ä»¥é™ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’è¡¨ç¤º
                # 2023å¹´ä»¥å‰ã®ãƒ‡ãƒ¼ã‚¿ã¯é™¤å¤–
                min_year = 2024
                filtered_cached_data = []
                for item in cached_data:
                    published_date_str = item.get('published_date')
                    if published_date_str:
                        try:
                            # ISOå½¢å¼ã®æ–‡å­—åˆ—ã‚’datetimeã«å¤‰æ›
                            if isinstance(published_date_str, str):
                                published_date = datetime.fromisoformat(published_date_str.replace('Z', '+00:00'))
                                # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³æƒ…å ±ã‚’å‰Šé™¤ã—ã¦æ¯”è¼ƒ
                                published_date = published_date.replace(tzinfo=None)
                                # 2024å¹´ä»¥é™ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’è¡¨ç¤º
                                if published_date.year >= min_year:
                                    filtered_cached_data.append(item)
                        except Exception as e:
                            logger.debug(f"âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ã®æ—¥ä»˜ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {published_date_str} - {e}")
                            # æ—¥ä»˜ãŒãƒ‘ãƒ¼ã‚¹ã§ããªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                            continue
                
                if len(filtered_cached_data) == 0:
                    logger.warning(f"âš ï¸ CNN: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«{min_year}å¹´ä»¥é™ã®è¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“ã€‚NewsAPIã‚’å‘¼ã³å‡ºã—ã¾ã™")
                    return self._fetch_cnn_trends(limit)
                
                # å…¬é–‹æ—¥ã§ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„é †ï¼‰
                filtered_cached_data.sort(key=lambda x: x.get('published_date') or '', reverse=True)
                
                logger.info(f"âœ… CNN: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰{len(filtered_cached_data)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸï¼ˆé‡è¤‡æ’é™¤ãƒ»æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œï¼‰")
                return {
                    'success': True,
                    'data': filtered_cached_data[:limit],  # åˆ¶é™æ•°ã¾ã§å–å¾—
                    'status': 'cached',
                    'source': 'database_cache'
                }
            else:
                logger.warning("âš ï¸ CNN: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚NewsAPIã‚’å‘¼ã³å‡ºã—ã¾ã™")
                return self._fetch_cnn_trends(limit)
        
        except Exception as e:
            logger.error(f"âŒ CNN ãƒˆãƒ¬ãƒ³ãƒ‰å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return {'error': f'CNNãƒ‹ãƒ¥ãƒ¼ã‚¹ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}', 'success': False}
    
    def _fetch_cnn_trends(self, limit=25):
        """CNNãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆNewsAPIã®ã¿ï¼‰"""
        if not self.news_api_key:
            logger.error("âŒ NEWS_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚CNNè¨˜äº‹ã‚’å–å¾—ã§ãã¾ã›ã‚“")
            return {
                'success': False,
                'error': 'NEWS_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“',
                'data': []
            }
        return self._fetch_cnn_from_newsapi(limit)
    
    def _fetch_cnn_from_newsapi(self, limit=25):
        """NewsAPIã‹ã‚‰CNNã®æœ€æ–°è¨˜äº‹ã‚’å–å¾—"""
        try:
            logger.info(f"ğŸ“° NewsAPIã‹ã‚‰CNNè¨˜äº‹ã‚’å–å¾—é–‹å§‹")
            
            # NewsAPIã®everythingã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã§CNNã‚½ãƒ¼ã‚¹ã‚’æŒ‡å®š
            # è‹±èªç‰ˆã®CNNè¨˜äº‹ã‚’å„ªå…ˆçš„ã«å–å¾—
            url = f"{self.news_api_base_url}/everything"
            params = {
                'sources': 'cnn',
                'pageSize': limit,
                'sortBy': 'publishedAt',
                'language': 'en',  # è‹±èªç‰ˆã‚’å„ªå…ˆ
                'apiKey': self.news_api_key
            }
            
            logger.debug(f"NewsAPIãƒªã‚¯ã‚¨ã‚¹ãƒˆ: {url} (sources=cnn)")
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’ãƒã‚§ãƒƒã‚¯
            self.rate_limiter.wait_if_needed()
            
            response = requests.get(url, params=params, timeout=10)
            
            if response.status_code != 200:
                logger.error(f"âŒ NewsAPI ã‚¨ãƒ©ãƒ¼: HTTP {response.status_code}")
                logger.error(f"ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response.text}")
                return {
                    'success': False,
                    'error': f'NewsAPI ã‚¨ãƒ©ãƒ¼: HTTP {response.status_code}',
                    'data': []
                }
            
            data = response.json()
            
            if data.get('status') != 'ok':
                error_msg = data.get('message', 'Unknown error')
                logger.error(f"âŒ NewsAPI ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¨ãƒ©ãƒ¼: {error_msg}")
                return {
                    'success': False,
                    'error': f'NewsAPI ã‚¨ãƒ©ãƒ¼: {error_msg}',
                    'data': []
                }
            
            articles = data.get('articles', [])
            logger.info(f"âœ… NewsAPIã‹ã‚‰CNNè¨˜äº‹ã‚’{len(articles)}ä»¶å–å¾—")
            
            if len(articles) == 0:
                logger.warning("âš ï¸ NewsAPIã§CNNè¨˜äº‹ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                return {
                    'success': True,
                    'data': [],
                    'status': 'no_articles',
                    'source': 'newsapi',
                    'message': 'CNNè¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ'
                }
            
            # NewsAPIã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’CNNå½¢å¼ã«å¤‰æ›
            formatted_data = []
            for i, article in enumerate(articles, 1):
                formatted_data.append({
                    'rank': i,
                    'title': article.get('title', 'No Title'),
                    'url': article.get('url', ''),
                    'published_date': article.get('publishedAt', ''),
                    'description': article.get('description', ''),
                    'source': 'CNN (via NewsAPI)'
                })
            
            # å…¬é–‹æ—¥ã§ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„é †ï¼‰
            formatted_data.sort(key=lambda x: x.get('published_date') or '', reverse=True)
            
            # åˆ¶é™æ•°ã¾ã§å–å¾—
            formatted_data = formatted_data[:limit]
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            self.db.save_cnn_trends_to_cache(formatted_data)
            logger.info(f"âœ… CNN: NewsAPIã‹ã‚‰{len(formatted_data)}ä»¶ã®æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’å–å¾—ã—ã¾ã—ãŸ")
            
            return {
                'success': True,
                'data': formatted_data,
                'status': 'api_fetched',
                'source': 'newsapi'
            }
            
        except requests.exceptions.Timeout:
            logger.error("âŒ NewsAPIã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼", exc_info=True)
            return {
                'success': False,
                'error': 'NewsAPIã‹ã‚‰ã®å¿œç­”ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ',
                'data': []
            }
        except Exception as e:
            logger.error(f"âŒ NewsAPI ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return {
                'success': False,
                'error': f'NewsAPI ã‚¨ãƒ©ãƒ¼: {str(e)}',
                'data': []
            }

    def _remove_duplicates(self, items):
        """é‡è¤‡ã‚’æ’é™¤ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰"""
        def normalize_title(title):
            """ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ­£è¦åŒ–ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰"""
            if not title:
                return ''
            normalized = str(title).strip()
            normalized = re.sub(r'\s+', ' ', normalized)
            return normalized
        
        seen_urls = set()
        seen_titles = set()
        unique_items = []
        duplicate_count = 0
        
        for item in items:
            url = str(item.get('url', '')).strip()
            title = str(item.get('title', '')).strip()
            normalized_title = normalize_title(title)
            
            # URLã¾ãŸã¯æ­£è¦åŒ–ã•ã‚ŒãŸã‚¿ã‚¤ãƒˆãƒ«ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if url in seen_urls or normalized_title in seen_titles:
                duplicate_count += 1
                continue
            
            # ç©ºã®ã‚¿ã‚¤ãƒˆãƒ«ã‚„URLã¯ã‚¹ã‚­ãƒƒãƒ—
            if not normalized_title or not url:
                duplicate_count += 1
                continue
            
            seen_urls.add(url)
            seen_titles.add(normalized_title)
            unique_items.append(item)
        
        if duplicate_count > 0:
            logger.info(f"ğŸ”„ CNN: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰{duplicate_count}ä»¶ã®é‡è¤‡ã‚’æ’é™¤ã—ã¾ã—ãŸï¼ˆæ®‹ã‚Š: {len(unique_items)}ä»¶ï¼‰")
        
        return unique_items

