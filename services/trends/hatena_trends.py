import os
import requests
import json
import feedparser
import urllib.parse
from datetime import datetime, timedelta
from database_config import TrendsCache
from utils.logger_config import get_logger

# ãƒ­ã‚¬ãƒ¼ã®åˆæœŸåŒ–
logger = get_logger(__name__)

class HatenaTrendsManager:
    """ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ãƒˆãƒ¬ãƒ³ãƒ‰ç®¡ç†ã‚¯ãƒ©ã‚¹ï¼ˆå…¬å¼RSS + APIä½¿ç”¨ï¼‰"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.base_url = "https://b.hatena.ne.jp"
        self.count_api_url = "https://bookmark.hatenaapis.com/count/entry"
        self.entry_api_url = "https://b.hatena.ne.jp/entry/json"
        self.db = TrendsCache()
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™: ã¯ã¦ãªAPIã¯ç‰¹ã«åˆ¶é™ãªã—ã ãŒã€ä¿å®ˆçš„ã«10ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/åˆ†ã«è¨­å®š
        self.rate_limiter = get_rate_limiter('hatena', max_requests=10, window_seconds=60)
        
        logger.info(f"ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ Trends ManageråˆæœŸåŒ–:")
        logger.info(f"  ãƒ›ãƒƒãƒˆã‚¨ãƒ³ãƒˆãƒªãƒ¼RSS: {self.base_url}/hotentry.rss")
        logger.info(f"  Count API: {self.count_api_url}")
        logger.info(f"  Entry API: {self.entry_api_url}")
    
    def get_trends(self, category='all', limit=25, force_refresh=False, fetch_all_categories=False):
        """ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’å–å¾—ï¼ˆget_hot_entriesã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼‰"""
        logger.debug(f"ğŸ” ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯: get_trendså‘¼ã³å‡ºã— (category: {category}, fetch_all_categories: {fetch_all_categories})")
        
        # å…¨ã‚«ãƒ†ã‚´ãƒªã‚’å–å¾—ã™ã‚‹å ´åˆ
        if fetch_all_categories:
            logger.info("ğŸ”„ ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯: å…¨ã‚«ãƒ†ã‚´ãƒªã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã™")
            all_data = self._fetch_and_cache_all_categories()
            if all_data:
                self._save_all_categories_to_cache(all_data)
                # 'all'ã‚«ãƒ†ã‚´ãƒªã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
                all_category_data = [item for item in all_data if item.get('category') == 'all']
                return {
                    'data': all_category_data[:limit] if all_category_data else [],
                    'status': 'api_fetched',
                    'category': 'all',
                    'source': 'Hatena API',
                    'success': True
                }
            else:
                return {
                    'data': [],
                    'status': 'api_error',
                    'category': 'all',
                    'error': 'å…¨ã‚«ãƒ†ã‚´ãƒªã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ',
                    'success': False
                }
        
        result = self.get_hot_entries(category, limit, force_refresh)
        logger.debug(f"ğŸ” ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯: get_trendså®Œäº† (category: {category})")
        return result
    
    def get_hot_entries(self, category='all', limit=25, force_refresh=False):
        """ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã®ãƒ›ãƒƒãƒˆã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚’å–å¾—ï¼ˆã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰"""
        try:
            force_fetch = force_refresh
            logger.debug(f"ğŸ” ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹ (category: {category})")
            
            cached_data = None
            if force_fetch:
                logger.info(f"ğŸ”„ ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯: force_refreshæŒ‡å®šã®ãŸã‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ (category: {category})")
            else:
                # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—ã‚’è©¦è¡Œ
                logger.debug(f"ğŸ” ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯: get_from_cache_by_categoryå‘¼ã³å‡ºã—é–‹å§‹")
                cached_data = self.get_from_cache_by_category(category)
                logger.debug(f"ğŸ” ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯: get_from_cache_by_categoryå‘¼ã³å‡ºã—å®Œäº†")
                logger.debug(f"ğŸ” ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿å–å¾—çµæœ: {type(cached_data)}, é•·ã•: {len(cached_data) if cached_data else 0}")
            
            if cached_data and len(cached_data) > 0:
                # ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°ã§ã‚½ãƒ¼ãƒˆï¼ˆé™é †ï¼‰
                cached_data.sort(key=lambda x: x.get('bookmark_count', 0), reverse=True)
                
                # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’å†è¨­å®š
                for i, item in enumerate(cached_data, 1):
                    item['rank'] = i
                
                logger.info(f"âœ… ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã€ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°ã§ã‚½ãƒ¼ãƒˆã—ã¾ã—ãŸ ({len(cached_data)}ä»¶)")
                cache_info = self._get_cache_info('hatena_trends')
                return {
                    'data': cached_data,
                    'status': 'cached',
                    'category': category,
                    'cache_info': cache_info,
                    'source': 'database_cache',
                    'success': True
                }
            
            # force_refresh=Falseã®å ´åˆã§ã‚‚ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒãªã„å ´åˆã¯å¤–éƒ¨APIã‚’å‘¼ã³å‡ºã™
            # ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠã—ãŸã¨ãã«ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤ºã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ãŸã‚ï¼‰
            if not force_fetch:
                logger.warning(f"âš ï¸ ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å¤–éƒ¨APIã‚’å‘¼ã³å‡ºã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã™ (category: {category})")
            
            logger.warning(f"âš ï¸ ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯: ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœªä½¿ç”¨ã®ãŸã‚å¤–éƒ¨APIã‚’å‘¼ã³å‡ºã—ã¾ã™")
            # æŒ‡å®šã‚«ãƒ†ã‚´ãƒªã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’å–å¾—
            api_result = self.get_new_entries(category, limit)
            
            if api_result and not api_result.get('error'):
                trends_data = api_result.get('data', [])
                # ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚’è¿½åŠ 
                for item in trends_data:
                    item['category'] = category
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ï¼ˆä»–ã®ã‚«ãƒ†ã‚´ãƒªã¨åŒã˜æ–¹æ³•ï¼‰
                success = self.db.save_hatena_trends_to_cache(trends_data, category)
                if success:
                    logger.info(f"âœ… ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯: å¤–éƒ¨APIã‹ã‚‰{len(trends_data)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ã—ã¾ã—ãŸ")
                else:
                    logger.warning(f"âš ï¸ ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯: ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸã—ã¾ã—ãŸãŒã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")
                
                return {
                    'data': trends_data,
                    'status': 'api_fetched',
                    'category': category,
                    'source': 'Hatena API',
                    'success': True
                }
            else:
                logger.error(f"âŒ ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯: å¤–éƒ¨APIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                return {
                    'data': [],
                    'status': 'api_error',
                    'category': category,
                    'error': api_result.get('error', 'Unknown error') if api_result else 'API call failed',
                    'success': False
                }
            
            # å…¨ã¦å¤±æ•—ã—ãŸå ´åˆ
            error_msg = f"ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯: ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ (category: {category})"
            logger.error(f"âŒ {error_msg}")
            return {
                'data': [],
                'status': 'api_error',
                'category': category,
                'error': error_msg,
                'success': False
            }
                
        except Exception as e:
            import traceback
            error_msg = f'ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ãƒˆãƒ¬ãƒ³ãƒ‰å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}'
            logger.error(f"âŒ ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯: ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            traceback.print_exc()
            return {
                'error': error_msg,
                'status': 'api_error',
                'category': category,
                'success': False
            }
    
    def get_new_entries(self, category='all', limit=25):
        """ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã®æ–°ç€ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚’å–å¾—ï¼ˆå…¬å¼RSSä½¿ç”¨ï¼‰"""
        try:
            # ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥æ–°ç€RSS URLã‚’æ§‹ç¯‰
            if category == 'all':
                rss_url = f"{self.base_url}/hotentry.rss"  # ãƒ›ãƒƒãƒˆã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚’ä½¿ç”¨
            else:
                rss_url = f"{self.base_url}/entrylist/{category}.rss"
            
            logger.info(f"ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ–°ç€ã‚¨ãƒ³ãƒˆãƒªãƒ¼RSSå–å¾—é–‹å§‹: {rss_url}")
            
            # RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—
            feed = feedparser.parse(rss_url)
            
            if not feed.entries:
                return {'error': 'RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ'}
            
            # ã‚¨ãƒ³ãƒˆãƒªãƒ¼æƒ…å ±ã‚’æŠ½å‡º
            items = []
            for entry in feed.entries[:limit]:
                # å…¬é–‹æ—¥æ™‚ã‚’é©åˆ‡ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
                published = entry.get('published', '') or entry.get('updated', '') or entry.get('created', '')
                if published:
                    try:
                        from datetime import datetime
                        import email.utils
                        # RFC 2822å½¢å¼ã®æ—¥ä»˜ã‚’ãƒ‘ãƒ¼ã‚¹
                        parsed_date = email.utils.parsedate_tz(published)
                        if parsed_date:
                            dt = datetime(*parsed_date[:6])
                            published = dt.strftime('%Y-%m-%d %H:%M:%S')
                    except:
                        # ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆã¯å…ƒã®æ–‡å­—åˆ—ã‚’ä½¿ç”¨
                        published = published
                else:
                    # æ—¥ä»˜ãŒå–å¾—ã§ããªã„å ´åˆã¯ç¾åœ¨æ™‚åˆ»ã‚’ä½¿ç”¨
                    from datetime import datetime
                    published = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                
                # entry_idã‚’ç”Ÿæˆï¼ˆURLã®ãƒãƒƒã‚·ãƒ¥åŒ–ï¼‰
                import hashlib
                entry_url = entry.get('link', '')
                entry_id = hashlib.md5(entry_url.encode('utf-8')).hexdigest() if entry_url else ''
                
                item = {
                    'entry_id': entry_id,
                    'title': entry.get('title', ''),
                    'url': entry.get('link', ''),
                    'description': entry.get('summary', ''),
                    'published': published,
                    'author': entry.get('author', ''),
                    'category': category
                }
                
                # ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°ã‚’å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
                item['bookmark_count'] = self._get_bookmark_count(item['url'])
                items.append(item)
            
            # ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°ã§ã‚½ãƒ¼ãƒˆï¼ˆé™é †ï¼‰
            items.sort(key=lambda x: x.get('bookmark_count', 0), reverse=True)
            
            # ãƒ©ãƒ³ã‚¯ã‚’ä»˜ä¸ï¼ˆãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°ã§ã‚½ãƒ¼ãƒˆå¾Œï¼‰
            trends_data = []
            for i, item in enumerate(items):
                trends_data.append({
                    'entry_id': item['entry_id'],
                    'rank': i + 1,
                    'title': item['title'],
                    'url': item['url'],
                    'description': item['description'],
                    'bookmark_count': item['bookmark_count'],
                    'published': item['published'],
                    'author': item['author'],
                    'category': item['category']
                })
            
            return {
                'data': trends_data,
                'status': 'success',
                'source': f'ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ–°ç€ã‚¨ãƒ³ãƒˆãƒªãƒ¼ï¼ˆ{category}ï¼‰',
                'total_count': len(trends_data),
                'category': category
            }
                
        except Exception as e:
            import traceback
            error_msg = f'ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ–°ç€ã‚¨ãƒ³ãƒˆãƒªãƒ¼å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}'
            logger.error(f"âŒ {error_msg}")
            traceback.print_exc()
            return {
                'error': error_msg,
                'data': [],
                'status': 'api_error',
                'success': False
            }
    
    def _get_bookmark_count(self, url):
        """ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯Count APIã§ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°ã‚’å–å¾—"""
        try:
            params = {'url': url}
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’ãƒã‚§ãƒƒã‚¯
            self.rate_limiter.wait_if_needed()
            
            response = requests.get(self.count_api_url, params=params, timeout=5)
            
            if response.status_code == 200:
                # è¿”ã‚Šå€¤ã¯æ•°å€¤ or "null"ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆï¼‰
                try:
                    count_text = response.text.strip()
                    if count_text.isdigit():
                        return int(count_text)
                    else:
                        return 0
                except:
                    return 0
            else:
                return 0
                
        except Exception as e:
            logger.error(f"ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return 0
    
    def get_entry_details(self, url):
        """ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯Entry APIã§ã‚¨ãƒ³ãƒˆãƒªãƒ¼è©³ç´°ã‚’å–å¾—"""
        try:
            params = {'url': url}
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’ãƒã‚§ãƒƒã‚¯
            self.rate_limiter.wait_if_needed()
            
            response = requests.get(self.entry_api_url, params=params, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                return {
                    'title': data.get('title', ''),
                    'url': data.get('url', ''),
                    'bookmarks': data.get('bookmarks', []),
                    'tags': data.get('tags', []),
                    'screenshot': data.get('screenshot', ''),
                    'eid': data.get('eid', '')
                }
            else:
                return None
                
        except Exception as e:
            logger.error(f"ã‚¨ãƒ³ãƒˆãƒªãƒ¼è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return None
    
    def get_available_categories(self):
        """åˆ©ç”¨å¯èƒ½ãªã‚«ãƒ†ã‚´ãƒªãƒ¼ä¸€è¦§ã‚’å–å¾—ï¼ˆäººæ°—ã®5ã‚«ãƒ†ã‚´ãƒªã«çµã‚Šè¾¼ã¿ï¼‰"""
        return [
            'all',           # ç·åˆ
            'it',            # ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ï¼ˆæœ€ã‚‚äººæ°—ï¼‰
            'social',        # ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»ç¤¾ä¼šï¼ˆæ–‡åŒ–ã€äº‹ä»¶ã€æ™‚äº‹ï¼‰
            'entertainment', # ã‚¨ãƒ³ã‚¿ãƒ¡ï¼ˆã‚¹ãƒãƒ¼ãƒ„ã€èŠ¸èƒ½ã€éŸ³æ¥½ã€æ˜ ç”»ï¼‰
            'life',          # æš®ã‚‰ã—ï¼ˆè¡£é£Ÿä½ã€æ‹æ„›ã€äººé–“é–¢ä¿‚ã€æ‚©ã¿ï¼‰
            'knowledge'      # å­¦ã³ï¼ˆç§‘å­¦æŠ€è¡“ã€å­¦å•ã€å­¦ç¿’ï¼‰
        ]
    
    def get_hatena_trends_summary(self):
        """ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ãƒˆãƒ¬ãƒ³ãƒ‰ã®æ¦‚è¦ã‚’å–å¾—"""
        return {
            'hatena_api': {
                'available': True,
                'note': 'ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯å…¬å¼RSS + API: ãƒ›ãƒƒãƒˆã‚¨ãƒ³ãƒˆãƒªãƒ¼ã€æ–°ç€ã‚¨ãƒ³ãƒˆãƒªãƒ¼',
                'features': [
                    'ãƒ›ãƒƒãƒˆã‚¨ãƒ³ãƒˆãƒªãƒ¼å–å¾—ï¼ˆRSSï¼‰',
                    'æ–°ç€ã‚¨ãƒ³ãƒˆãƒªãƒ¼å–å¾—ï¼ˆRSSï¼‰',
                    'ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥åˆ†é¡',
                    'ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°å–å¾—ï¼ˆCount APIï¼‰',
                    'ã‚¨ãƒ³ãƒˆãƒªãƒ¼è©³ç´°å–å¾—ï¼ˆEntry APIï¼‰',
                    'å…¬å¼APIä½¿ç”¨ï¼ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãªã—ï¼‰'
                ]
            },
            'limitations': [
                'RSSæ›´æ–°é »åº¦ã«ä¾å­˜',
                'ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚ã‚Š',
                'ã‚«ãƒ†ã‚´ãƒªãƒ¼æ•°ãŒå¤šã„'
            ],
            'setup_required': [
                'feedparserãƒ©ã‚¤ãƒ–ãƒ©ãƒª',
                'å…¬å¼RSS + APIä½¿ç”¨'
            ]
        }
    
    def get_from_cache(self, cache_key):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        try:
            return self.db.get_from_cache('hatena_trends', 'hatena_trends')
        except Exception as e:
            logger.error(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return None
    
    def get_from_cache_by_category(self, category):
        """ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        try:
            logger.debug(f"ğŸ” ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—: category='{category}'")
            # ä»–ã®ã‚«ãƒ†ã‚´ãƒªã¨åŒã˜ã‚ˆã†ã«ã€TrendsCacheã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
            # regionãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«categoryã‚’æ¸¡ã™ã“ã¨ã§ã€ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«å–å¾—
            cached_data = self.db.get_hatena_trends_from_cache(category)
            
            if cached_data:
                # categoryã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆå¿µã®ãŸã‚ï¼‰
                category_data = [item for item in cached_data if item.get('category') == category]
                logger.info(f"âœ… ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—å®Œäº†: {len(category_data)}ä»¶")
                if len(category_data) > 0:
                    logger.debug(f"ğŸ” æœ€åˆã®ã‚¢ã‚¤ãƒ†ãƒ ã®ã‚«ãƒ†ã‚´ãƒª: {category_data[0].get('category', 'N/A')}")
                return category_data
            else:
                logger.warning(f"âš ï¸ ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—: ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                return []
                    
        except Exception as e:
            logger.error(f"âŒ ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            import traceback
            traceback.print_exc()
            return []
    
    def _fetch_and_cache_all_categories(self):
        """å…¨ã‚«ãƒ†ã‚´ãƒªã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€åº¦ã«å–å¾—ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        try:
            logger.info("ğŸ”„ ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯: å…¨ã‚«ãƒ†ã‚´ãƒªã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—é–‹å§‹")
            
            # åˆ©ç”¨å¯èƒ½ãªã‚«ãƒ†ã‚´ãƒªã‚’å–å¾—
            categories = self.get_available_categories()
            all_data = []
            
            for category in categories:
                logger.info(f"ğŸ“Š ã‚«ãƒ†ã‚´ãƒª '{category}' ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
                api_result = self.get_new_entries(category, 25)
                logger.debug(f"ğŸ” ã‚«ãƒ†ã‚´ãƒª '{category}' APIçµæœ: {api_result}")
                if api_result and not api_result.get('error'):
                    trends_data = api_result.get('data', [])
                    for item in trends_data:
                        item['category'] = category
                    all_data.extend(trends_data)
                    logger.info(f"âœ… ã‚«ãƒ†ã‚´ãƒª '{category}': {len(trends_data)}ä»¶å–å¾—")
                else:
                    logger.warning(f"âŒ ã‚«ãƒ†ã‚´ãƒª '{category}': ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•— - {api_result}")
            
            # 'all'ã‚«ãƒ†ã‚´ãƒªã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ï¼ˆãƒ›ãƒƒãƒˆã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‹ã‚‰å–å¾—ï¼‰
            logger.info(f"ğŸ“Š ã‚«ãƒ†ã‚´ãƒª 'all' ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
            api_result = self.get_new_entries('all', 25)
            if api_result and not api_result.get('error'):
                trends_data = api_result.get('data', [])
                for item in trends_data:
                    item['category'] = 'all'
                all_data.extend(trends_data)
                logger.info(f"âœ… ã‚«ãƒ†ã‚´ãƒª 'all': {len(trends_data)}ä»¶å–å¾—")
            
            # ãƒ‡ãƒ¼ã‚¿å–å¾—ã¯æˆåŠŸï¼ˆä¿å­˜ã¯å‘¼ã³å‡ºã—å…ƒã§è¡Œã†ï¼‰
            if all_data:
                logger.info(f"âœ… ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯: å…¨ã‚«ãƒ†ã‚´ãƒªã®ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº† ({len(all_data)}ä»¶)")
            else:
                logger.warning("âŒ ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯: å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                
        except Exception as e:
            import traceback
            logger.error(f"âŒ å…¨ã‚«ãƒ†ã‚´ãƒªå–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            traceback.print_exc()
            all_data = []
        
        # å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™ï¼ˆä¿å­˜å‡¦ç†ã¯å‘¼ã³å‡ºã—å…ƒã§è¡Œã†ï¼‰
        return all_data
    
    def _save_all_categories_to_cache(self, all_data):
        """å…¨ã‚«ãƒ†ã‚´ãƒªã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        if not all_data:
            return 0
        
        try:
            # é‡è¤‡æ’é™¤
            seen = set()
            unique_data = []
            for item in all_data:
                category = item.get('category', '')
                title = item.get('title', '')
                url = item.get('url', '')
                dedupe_key = (category, title, url)
                if dedupe_key in seen:
                    continue
                seen.add(dedupe_key)
                unique_data.append(item)
            
            # ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦ä¿å­˜
            saved_count = 0
            categories = self.get_available_categories()
            for category in categories:
                category_data = [item for item in unique_data if item.get('category') == category]
                if category_data:
                    # ä»–ã®ã‚«ãƒ†ã‚´ãƒªã¨åŒã˜ã‚ˆã†ã«ã€TrendsCacheã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
                    # regionãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«categoryã‚’æ¸¡ã™ã“ã¨ã§ã€ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«ä¿å­˜
                    success = self.db.save_hatena_trends_to_cache(category_data, category)
                    if success:
                        saved_count += len(category_data)
                        logger.info(f"âœ… ã‚«ãƒ†ã‚´ãƒª '{category}': {len(category_data)}ä»¶ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ã—ã¾ã—ãŸ")
            
            if saved_count > 0:
                logger.info(f"âœ… ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯: å…¨ã‚«ãƒ†ã‚´ãƒªã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜å®Œäº† ({saved_count}ä»¶)")
                # cache_statusã‚’æ›´æ–°
                self._update_cache_status('hatena_trends', saved_count)
            
            return saved_count
            
        except Exception as e:
            logger.error(f"âŒ å…¨ã‚«ãƒ†ã‚´ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            import traceback
            traceback.print_exc()
            return 0
    
    def save_to_cache(self, data, cache_key):
        """ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        try:
            # åŒä¸€ã‚«ãƒ†ã‚´ãƒªå†…ã§ã®é‡è¤‡ã‚’æ’é™¤
            unique_data = []
            seen = set()
            for item in data or []:
                category = item.get('category', cache_key)
                title = item.get('title', '')
                url = item.get('url', '')
                dedupe_key = (category, title, url)
                if dedupe_key in seen:
                    continue
                seen.add(dedupe_key)
                unique_data.append(item)

            self.db.save_hatena_trends_to_cache(unique_data, cache_key)
            # cache_statusãƒ†ãƒ¼ãƒ–ãƒ«ã‚‚æ›´æ–°
            self._update_cache_status('hatena_trends', len(unique_data))
        except Exception as e:
            logger.error(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
    
    def is_cache_valid(self, cache_key):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯"""
        try:
            return self.db.is_hatena_cache_valid(cache_key)
        except Exception as e:
            logger.error(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return False
    
    def _should_refresh_cache(self, cache_key):
        """ä»Šæ—¥æ—¢ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–°ã—ãŸã‹ãƒã‚§ãƒƒã‚¯ï¼ˆæœ5æ™‚ã‹ã‚‰å¤œ12æ™‚ã¾ã§ï¼‰"""
        try:
            from datetime import datetime
            import pytz
            # æ—¥æœ¬æ™‚é–“ã§ç¾åœ¨æ™‚åˆ»ã‚’å–å¾—
            jst = pytz.timezone('Asia/Tokyo')
            now = datetime.now(jst)
            today = now.date()
            current_hour = now.hour
            
            # æ™‚é–“åˆ¶é™ï¼š5æ™‚ã‹ã‚‰24æ™‚ã¾ã§
            if not (5 <= current_hour < 24):
                logger.info(f"âš ï¸ æ™‚é–“å¤–ã§ã™ï¼ˆ{current_hour}æ™‚ï¼‰ã€‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                return False
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æœ€å¾Œã®æ›´æ–°æ—¥æ™‚ã‚’å–å¾—
            conn = self.db.get_connection()
            if not conn:
                logger.warning("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                return False
            
            with conn:
                with conn.cursor() as cursor:
                    cursor.execute("""
                        SELECT last_updated 
                        FROM cache_status 
                        WHERE cache_key = %s
                    """, ('hatena_trends',))
                    
                    result = cursor.fetchone()
                    if result and result[0]:
                        last_refresh = result[0].date()
                        return last_refresh < today
                    return True  # åˆå›ã¯æ›´æ–°ã™ã‚‹
        except Exception as e:
            logger.error(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°æ—¥æ™‚ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return True
    
    def _update_refresh_time(self, cache_key):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°æ—¥æ™‚ã‚’è¨˜éŒ²"""
        try:
            from datetime import datetime
            import pytz
            # æ—¥æœ¬æ™‚é–“ã§ç¾åœ¨æ™‚åˆ»ã‚’å–å¾—
            jst = pytz.timezone('Asia/Tokyo')
            now = datetime.now(jst)
            conn = self.db.get_connection()
            if not conn:
                logger.warning("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚æ›´æ–°æ—¥æ™‚è¨˜éŒ²ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                return
            
            with conn:
                with conn.cursor() as cursor:
                    cursor.execute("""
                        INSERT INTO cache_status (cache_key, last_updated, data_count)
                        VALUES (%s, %s, %s)
                        ON CONFLICT (cache_key) 
                        DO UPDATE SET 
                            last_updated = EXCLUDED.last_updated,
                            data_count = EXCLUDED.data_count
                    """, ('hatena_trends', now, 25))  # æ­£ã—ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ä½¿ç”¨
                    conn.commit()
        except Exception as e:
            logger.error(f"æ›´æ–°æ—¥æ™‚è¨˜éŒ²ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
    
    def _update_cache_status(self, cache_key, data_count):
        """cache_statusãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ›´æ–°"""
        try:
            from datetime import datetime
            now = datetime.now()
            
            conn = self.db.get_connection()
            if not conn:
                logger.warning("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚cache_statusæ›´æ–°ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                return
            
            with conn:
                with conn.cursor() as cursor:
                    cursor.execute("""
                        INSERT INTO cache_status (cache_key, last_updated, data_count)
                        VALUES (%s, %s, %s)
                        ON CONFLICT (cache_key) DO UPDATE SET
                            last_updated = EXCLUDED.last_updated,
                            data_count = EXCLUDED.data_count
                    """, (cache_key, now, data_count))
                    conn.commit()
        except Exception as e:
            logger.error(f"cache_statusæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)

    def _get_cache_info(self, cache_key):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥æƒ…å ±ã‚’å–å¾—"""
        try:
            conn = self.db.get_connection()
            if not conn:
                logger.warning("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥æƒ…å ±å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                return {'last_updated': None, 'data_count': 0}
            
            with conn:
                with conn.cursor() as cursor:
                    cursor.execute("""
                        SELECT last_updated, data_count 
                        FROM cache_status 
                        WHERE cache_key = %s
                    """, ('hatena_trends',))
                    
                    result = cursor.fetchone()
                    if result:
                        return {
                            'last_updated': result[0].isoformat() if result[0] else None,
                            'data_count': result[1] or 0
                        }
                    return {'last_updated': None, 'data_count': 0}
        except Exception as e:
            logger.error(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return {'last_updated': None, 'data_count': 0}