import os
import re
import requests
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from database_config import TrendsCache
from utils.logger_config import get_logger
from utils.rate_limiter import get_rate_limiter

logger = get_logger(__name__)

class NHKTrendsManager:
    """NHK RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’å–å¾—ãƒ»ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        # NHK RSSãƒ•ã‚£ãƒ¼ãƒ‰URL
        self.rss_urls = {
            'main': 'https://www3.nhk.or.jp/rss/news/cat0.xml',  # ä¸»è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹
            'domestic': 'https://www3.nhk.or.jp/rss/news/cat1.xml',  # å›½å†…
            'international': 'https://www3.nhk.or.jp/rss/news/cat2.xml',  # å›½éš›
            'economy': 'https://www3.nhk.or.jp/rss/news/cat3.xml',  # çµŒæ¸ˆ
            'sports': 'https://www3.nhk.or.jp/rss/news/cat4.xml',  # ã‚¹ãƒãƒ¼ãƒ„
            'science': 'https://www3.nhk.or.jp/rss/news/cat5.xml',  # ç§‘å­¦ãƒ»æ–‡åŒ–
        }
        self.db = TrendsCache()
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™: NHK APIã¯ç‰¹ã«åˆ¶é™ãªã—ã ãŒã€ä¿å®ˆçš„ã«10ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/åˆ†ã«è¨­å®š
        self.rate_limiter = get_rate_limiter('nhk', max_requests=10, window_seconds=60)
        
        logger.info("NHK Trends ManageråˆæœŸåŒ–:")
        logger.info(f"  RSS URL: {self.rss_urls['main']}")
    
    def get_trends(self, limit=25, force_refresh=False):
        """NHKãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ãªã„å ´åˆã®ã¿å¤–éƒ¨APIã‚’å‘¼ã³å‡ºã—ï¼‰"""
        try:
            if force_refresh:
                logger.info(f"ğŸ”„ NHK force_refresh: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã™")
                self.db.clear_nhk_trends_cache()
            
            try:
                cached_data = self.db.get_nhk_trends_from_cache()
            except Exception as e:
                logger.error(f"âŒ NHK: ã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
                # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºã®ãƒªã‚¹ãƒˆã¨ã—ã¦æ‰±ã†ï¼ˆ500ã‚¨ãƒ©ãƒ¼ã‚’é˜²ãï¼‰
                cached_data = []
            
            if cached_data:
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã«ã‚‚é‡è¤‡æ’é™¤ã‚’é©ç”¨
                cached_data = self._remove_duplicates(cached_data)
                logger.info(f"âœ… NHK: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰{len(cached_data)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸï¼ˆé‡è¤‡æ’é™¤å¾Œï¼‰")
                return {
                    'success': True,
                    'data': cached_data[:limit],  # åˆ¶é™æ•°ã¾ã§å–å¾—
                    'status': 'cached',
                    'source': 'database_cache'
                }
            else:
                logger.warning("âš ï¸ NHK: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å¤–éƒ¨RSSã‚’å‘¼ã³å‡ºã—ã¾ã™")
                return self._fetch_nhk_trends(limit)
        
        except Exception as e:
            logger.error(f"âŒ NHK ãƒˆãƒ¬ãƒ³ãƒ‰å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return {'error': f'NHKãƒ‹ãƒ¥ãƒ¼ã‚¹ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}', 'success': False}
    
    def _remove_duplicates(self, items):
        """é‡è¤‡ã‚’æ’é™¤ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰"""
        def normalize_title(title):
            """ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ­£è¦åŒ–ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰"""
            if not title:
                return ''
            normalized = str(title).strip()
            normalized = re.sub(r'\s+', ' ', normalized)
            return normalized
        
        seen_urls = set()
        seen_titles = set()
        unique_items = []
        duplicate_count = 0
        
        for item in items:
            url = str(item.get('url', '')).strip()
            title = str(item.get('title', '')).strip()
            normalized_title = normalize_title(title)
            
            # URLã¾ãŸã¯æ­£è¦åŒ–ã•ã‚ŒãŸã‚¿ã‚¤ãƒˆãƒ«ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if url in seen_urls or normalized_title in seen_titles:
                duplicate_count += 1
                continue
            
            # ç©ºã®ã‚¿ã‚¤ãƒˆãƒ«ã‚„URLã¯ã‚¹ã‚­ãƒƒãƒ—
            if not normalized_title or not url:
                duplicate_count += 1
                continue
            
            seen_urls.add(url)
            seen_titles.add(normalized_title)
            unique_items.append(item)
        
        if duplicate_count > 0:
            logger.info(f"ğŸ”„ NHK: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰{duplicate_count}ä»¶ã®é‡è¤‡ã‚’æ’é™¤ã—ã¾ã—ãŸï¼ˆæ®‹ã‚Š: {len(unique_items)}ä»¶ï¼‰")
        
        return unique_items
    
    def _parse_rss_items(self, root):
        """RSS XMLã‹ã‚‰ã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒ‘ãƒ¼ã‚¹"""
        items = []
        for item in root.findall('.//item'):
            try:
                title = item.find('title')
                link = item.find('link')
                pub_date = item.find('pubDate')
                description = item.find('description')
                
                if title is not None and link is not None:
                    # å…¬é–‹æ—¥ã‚’ãƒ‘ãƒ¼ã‚¹
                    published_date = None
                    if pub_date is not None and pub_date.text:
                        try:
                            # RFC 822å½¢å¼ã®æ—¥ä»˜ã‚’ãƒ‘ãƒ¼ã‚¹
                            from email.utils import parsedate_to_datetime
                            published_date = parsedate_to_datetime(pub_date.text)
                        except Exception as e:
                            logger.debug(f"æ—¥ä»˜ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
                            published_date = datetime.now()
                    
                    items.append({
                        'title': title.text if title is not None else '',
                        'url': link.text if link is not None else '',
                        'published_date': published_date.isoformat() if published_date else None,
                        'description': description.text if description is not None else ''
                    })
            except Exception as e:
                logger.warning(f"NHK RSS ã‚¢ã‚¤ãƒ†ãƒ ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        return items
    
    def _fetch_nhk_trends(self, limit=25):
        """NHK RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        try:
            logger.info(f"NHK RSSå‘¼ã³å‡ºã—é–‹å§‹")
            
            all_items = []
            
            # 1. ä¸»è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆcat0ï¼‰ã‹ã‚‰å–å¾—
            try:
                url = self.rss_urls['main']
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’ãƒã‚§ãƒƒã‚¯
                self.rate_limiter.wait_if_needed()
                
                response = requests.get(url, timeout=10)
                if response.status_code == 200:
                    root = ET.fromstring(response.content)
                    items = self._parse_rss_items(root)
                    all_items.extend(items)
                    logger.info(f"âœ… ä¸»è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(items)}ä»¶å–å¾—")
            except Exception as e:
                logger.warning(f"âš ï¸ ä¸»è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            
            # 2. å›½å†…ï¼ˆcat1ï¼‰ã‹ã‚‰ãƒˆãƒƒãƒ—10ä»¶ã‚’å–å¾—
            try:
                url = self.rss_urls['domestic']
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’ãƒã‚§ãƒƒã‚¯
                self.rate_limiter.wait_if_needed()
                
                response = requests.get(url, timeout=10)
                if response.status_code == 200:
                    root = ET.fromstring(response.content)
                    items = self._parse_rss_items(root)
                    all_items.extend(items[:10])  # ãƒˆãƒƒãƒ—10ä»¶ã®ã¿
                    logger.info(f"âœ… å›½å†…: {len(items[:10])}ä»¶å–å¾—")
            except Exception as e:
                logger.warning(f"âš ï¸ å›½å†…å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            
            # 3. å›½éš›ï¼ˆcat2ï¼‰ã‹ã‚‰ãƒˆãƒƒãƒ—10ä»¶ã‚’å–å¾—
            try:
                url = self.rss_urls['international']
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’ãƒã‚§ãƒƒã‚¯
                self.rate_limiter.wait_if_needed()
                
                response = requests.get(url, timeout=10)
                if response.status_code == 200:
                    root = ET.fromstring(response.content)
                    items = self._parse_rss_items(root)
                    all_items.extend(items[:10])  # ãƒˆãƒƒãƒ—10ä»¶ã®ã¿
                    logger.info(f"âœ… å›½éš›: {len(items[:10])}ä»¶å–å¾—")
            except Exception as e:
                logger.warning(f"âš ï¸ å›½éš›å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            
            if len(all_items) == 0:
                logger.warning("NHK RSSã§è¨˜äº‹ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                return {
                    'error': 'NHK RSSã§è¨˜äº‹ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ',
                    'success': False
                }
            
            # é‡è¤‡æ’é™¤ï¼ˆå…±é€šãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ï¼‰
            unique_items = self._remove_duplicates(all_items)
            
            # å…¬é–‹æ—¥ã§ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„é †ï¼‰
            unique_items.sort(key=lambda x: x.get('published_date', ''), reverse=True)
            
            # åˆ¶é™æ•°ã¾ã§å–å¾—
            formatted_data = unique_items[:limit]
            
            # ãƒ©ãƒ³ã‚¯ã‚’è¿½åŠ 
            for i, item in enumerate(formatted_data, 1):
                item['rank'] = i
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            self.db.save_nhk_trends_to_cache(formatted_data)
            logger.info(f"âœ… NHK: {len(formatted_data)}ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’å–å¾—ã—ã¾ã—ãŸ")
            
            return {
                'success': True,
                'data': formatted_data,
                'status': 'api_fetched',
                'source': 'nhk_rss'
            }
        
        except requests.exceptions.Timeout:
            logger.error("âŒ NHK RSSã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼", exc_info=True)
            return {'error': 'NHK RSSã‹ã‚‰ã®å¿œç­”ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ', 'success': False}
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ NHK RSSãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return {'error': f'NHK RSSãƒªã‚¯ã‚¨ã‚¹ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}', 'success': False}
        except ET.ParseError as e:
            logger.error(f"âŒ NHK RSS XMLãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return {'error': f'NHK RSS XMLã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}', 'success': False}
        except Exception as e:
            logger.error(f"âŒ NHKãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return {'error': f'NHKãƒ‹ãƒ¥ãƒ¼ã‚¹ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}', 'success': False}

