"""
YouTube Trendsé–¢é€£ã®å‡¦ç†ã‚’ç®¡ç†ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
"""

import os
from datetime import datetime, timezone, timedelta
from googleapiclient.discovery import build
from database_config import TrendsCache
from utils.logger_config import get_logger

# ãƒ­ã‚¬ãƒ¼ã®åˆæœŸåŒ–
logger = get_logger(__name__)

class YouTubeTrendsManager:
    """YouTube Trendsã®ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.youtube_api_key = os.getenv('YOUTUBE_API_KEY')
        self.db = TrendsCache()
    
    def get_trends(self, region_code: str = 'JP', max_results: int = 25, force_refresh=False):
        """YouTubeã®ãƒˆãƒ¬ãƒ³ãƒ‰å‹•ç”»ã‚’å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ãªã„å ´åˆã®ã¿å¤–éƒ¨APIã‚’å‘¼ã³å‡ºã—ï¼‰"""
        try:
            cached_data = None
            if force_refresh:
                logger.info(f"ğŸ”„ YouTube: force_refreshæŒ‡å®šã®ãŸã‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ (region: {region_code})")
            else:
                logger.debug(f"ğŸ” YouTube: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹ (region: {region_code})")
                cached_data = self.db.get_youtube_trends_from_cache(region_code, 'trending')
                logger.debug(f"ğŸ” YouTube: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿å–å¾—çµæœ: {type(cached_data)}, é•·ã•: {len(cached_data) if cached_data else 0}")
            
            if cached_data:
                # è¦–è´å›æ•°ã§ã‚½ãƒ¼ãƒˆï¼ˆé™é †ï¼‰
                cached_data.sort(key=lambda x: x.get('view_count', 0), reverse=True)
                
                # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’å†è¨­å®š
                for i, item in enumerate(cached_data, 1):
                    item['rank'] = i
                
                logger.info(f"âœ… YouTube: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã€è¦–è´å›æ•°ã§ã‚½ãƒ¼ãƒˆã—ã¾ã—ãŸ ({len(cached_data)}ä»¶)")
                return {
                    'data': cached_data,
                    'status': 'cached',
                    'region_code': region_code
                }
            logger.warning(f"âš ï¸ YouTube: ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœªä½¿ç”¨ã®ãŸã‚å¤–éƒ¨APIã‚’å‘¼ã³å‡ºã—ã¾ã™")
            api_result = self._fetch_trending_from_api(region_code, max_results)
            if api_result and isinstance(api_result, dict) and api_result.get('data'):
                trends_data = api_result['data']
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                self.save_to_cache(region_code, 'trending', trends_data)
                logger.info(f"âœ… YouTube: å¤–éƒ¨APIã‹ã‚‰{len(trends_data)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ã—ã¾ã—ãŸ")
                return {
                    'data': trends_data,
                    'status': 'api_fetched',
                    'region_code': region_code,
                    'source': 'YouTube Data API'
                }
            else:
                logger.error(f"âŒ YouTube: å¤–éƒ¨APIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                return {
                    'data': [],
                    'status': 'api_error',
                    'region_code': region_code
                }
            
        except Exception as e:
            logger.error(f"YouTube ã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return {'error': f'YouTube ã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}'}

    def get_rising_trends(self, region_code: str = 'JP', max_results: int = 25, force_refresh: bool = False):
        """YouTubeã®æ€¥ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰å‹•ç”»ã‚’å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å„ªå…ˆï¼‰"""
        try:
            cached_data = None
            if force_refresh:
                logger.info(f"ğŸ”„ YouTubeæ€¥ä¸Šæ˜‡: force_refreshæŒ‡å®šã®ãŸã‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ (region: {region_code})")
            else:
                logger.debug(f"ğŸ” YouTubeæ€¥ä¸Šæ˜‡: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹ (region: {region_code})")
            
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—ã‚’è©¦è¡Œ
                cached_data = self.get_from_cache(region_code, 'rising')
                logger.debug(f"ğŸ” YouTubeæ€¥ä¸Šæ˜‡: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿å–å¾—çµæœ: {type(cached_data)}, é•·ã•: {len(cached_data) if cached_data else 0}")
            
            if cached_data:
                # æ€¥ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰ã®å ´åˆã¯ã€ãƒˆãƒ¬ãƒ³ãƒ‰ã‚¹ã‚³ã‚¢ã¾ãŸã¯è¦–è´å›æ•°ã§å†ã‚½ãƒ¼ãƒˆã—ã¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’è¨­å®š
                if any(item.get('trend_score') is not None for item in cached_data):
                    # ãƒˆãƒ¬ãƒ³ãƒ‰ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆï¼ˆé™é †ï¼‰- Noneå€¤ã‚’0ã¨ã—ã¦æ‰±ã†
                    cached_data.sort(key=lambda x: x.get('trend_score') or 0, reverse=True)
                else:
                    # è¦–è´å›æ•°ã§ã‚½ãƒ¼ãƒˆï¼ˆé™é †ï¼‰- Noneå€¤ã‚’0ã¨ã—ã¦æ‰±ã†
                    cached_data.sort(key=lambda x: x.get('view_count') or 0, reverse=True)
                # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’è¨­å®š
                for i, item in enumerate(cached_data, 1):
                    item['rank'] = i
                
                logger.info(f"âœ… YouTubeæ€¥ä¸Šæ˜‡: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ ({len(cached_data)}ä»¶)")
                return {
                    'data': cached_data,
                    'status': 'cached',
                    'region_code': region_code
                }
            
            logger.warning(f"âš ï¸ YouTubeæ€¥ä¸Šæ˜‡: ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœªä½¿ç”¨ã®ãŸã‚å¤–éƒ¨APIã‚’å‘¼ã³å‡ºã—ã¾ã™")
            return self._fetch_rising_trends_from_api(region_code, max_results)
                
        except Exception as e:
            logger.error(f"YouTubeæ€¥ä¸Šæ˜‡ ã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return {'error': f'YouTubeæ€¥ä¸Šæ˜‡ ã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}'}

    def _fetch_trending_from_api(self, region_code: str = 'JP', max_results: int = 25):
        """YouTubeã®äººæ°—å‹•ç”»ã‚’å¤–éƒ¨APIã‹ã‚‰å–å¾—"""
        if not self.youtube_api_key:
            return {'error': 'YouTube APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“'}
        
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’ãƒã‚§ãƒƒã‚¯
        self.rate_limiter.wait_if_needed()
        
        try:
            youtube = build('youtube', 'v3', developerKey=self.youtube_api_key)
            
            # äººæ°—å‹•ç”»ã‚’å–å¾—
            request = youtube.videos().list(
                part='snippet,statistics',
                chart='mostPopular',
                regionCode=region_code,
                maxResults=max_results
            )
            
            response = request.execute()
            
            if not response.get('items'):
                return {'error': 'å‹•ç”»ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ'}
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢
            trends = []
            for item in response['items']:
                try:
                    trends.append({
                        'title': item['snippet']['title'],
                        'channel_title': item['snippet']['channelTitle'],
                        'view_count': int(item['statistics'].get('viewCount', 0)),
                        'like_count': int(item['statistics'].get('likeCount', 0)),
                        'comment_count': int(item['statistics'].get('commentCount', 0)),
                        'published_at': item['snippet']['publishedAt'],
                        'video_id': item['id'],
                        'thumbnail_url': item['snippet']['thumbnails']['medium']['url'],
                        'description': item['snippet']['description'][:100] + '...' if len(item['snippet']['description']) > 100 else item['snippet']['description']
                    })
                except Exception as e:
                    logger.warning(f"å‹•ç”»ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
                    continue
            
            # è¦–è´å›æ•°ã§ã‚½ãƒ¼ãƒˆï¼ˆé™é †ï¼‰
            trends.sort(key=lambda x: x.get('view_count', 0), reverse=True)
            
            # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’è¨­å®š
            for i, trend in enumerate(trends, 1):
                trend['rank'] = i
            
            logger.info(f"âœ… YouTubeäººæ°—å‹•ç”»: å¤–éƒ¨APIã‹ã‚‰{len(trends)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€è¦–è´å›æ•°ã§ã‚½ãƒ¼ãƒˆã—ã¾ã—ãŸ")
            
            return {
                'data': trends,
                'status': 'api_fetched',
                'region_code': region_code
            }
            
        except Exception as e:
            logger.error(f"YouTube Data APIã§ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return {'error': f'YouTube Data APIã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}'}

    def _fetch_rising_trends_from_api(self, region_code: str = 'JP', max_results: int = 25):
        """YouTubeã®æ€¥ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰å‹•ç”»ã‚’å¤–éƒ¨APIã‹ã‚‰å–å¾—"""
        if not self.youtube_api_key:
            return {'error': 'YouTube APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“'}
        
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’ãƒã‚§ãƒƒã‚¯
        self.rate_limiter.wait_if_needed()
        
        try:
            youtube = build('youtube', 'v3', developerKey=self.youtube_api_key)
            
            # æœ€è¿‘ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸå‹•ç”»ã‚’æ¤œç´¢ï¼ˆæ€¥ä¸Šæ˜‡ã®ä»£ã‚ã‚Šï¼‰
            from datetime import datetime, timezone, timedelta
            seven_days_ago = (datetime.now(timezone.utc) - timedelta(days=7)).isoformat()
            
            request = youtube.search().list(
                part='snippet',
                type='video',
                order='date',  # æœ€æ–°é †
                regionCode=region_code,
                maxResults=max_results,
                publishedAfter=seven_days_ago  # éå»7æ—¥ä»¥å†…
            )
            
            response = request.execute()
            
            if not response.get('items'):
                return {'error': 'å‹•ç”»ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ'}
            
            # å‹•ç”»IDã‚’åé›†
            video_ids = [item['id']['videoId'] for item in response['items']]
            
            # å‹•ç”»ã®è©³ç´°æƒ…å ±ã‚’å–å¾—
            video_request = youtube.videos().list(
                part='snippet,statistics',
                id=','.join(video_ids)
            )
            video_response = video_request.execute()
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢ã—ã€ãƒˆãƒ¬ãƒ³ãƒ‰ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
            trends = []
            for item in video_response['items']:
                try:
                    # æŠ•ç¨¿æ—¥æ™‚ã‹ã‚‰çµŒéæ—¥æ•°ã‚’è¨ˆç®—
                    published_date = datetime.fromisoformat(
                        item['snippet']['publishedAt'].replace('Z', '+00:00')
                    )
                    days_since_published = max(1, (datetime.now(timezone.utc) - published_date).days)
                    
                    # è¦–è´å›æ•°å¯†åº¦ï¼ˆ1æ—¥ã‚ãŸã‚Šã®è¦–è´å›æ•°ï¼‰ã‚’è¨ˆç®—
                    view_count = int(item['statistics'].get('viewCount', 0))
                    view_density = view_count / days_since_published
                    
                    # ãƒˆãƒ¬ãƒ³ãƒ‰ã‚¹ã‚³ã‚¢ï¼ˆæ–°ã—ã„å‹•ç”»ã»ã©é«˜ã‚¹ã‚³ã‚¢ï¼‰
                    trend_score = view_density
                    if days_since_published <= 1:
                        trend_score *= 2.0  # 1æ—¥ä»¥å†…ã¯2å€
                    elif days_since_published <= 3:
                        trend_score *= 1.5  # 3æ—¥ä»¥å†…ã¯1.5å€
                    
                    trends.append({
                        'title': item['snippet']['title'],
                        'channel_title': item['snippet']['channelTitle'],
                        'view_count': view_count,
                        'like_count': int(item['statistics'].get('likeCount', 0)),
                        'comment_count': int(item['statistics'].get('commentCount', 0)),
                        'published_at': item['snippet']['publishedAt'],
                        'video_id': item['id'],
                        'thumbnail_url': item['snippet']['thumbnails']['medium']['url'],
                        'description': item['snippet']['description'][:100] + '...' if len(item['snippet']['description']) > 100 else item['snippet']['description'],
                        'days_since_published': days_since_published,
                        'view_density': view_density,
                        'trend_score': trend_score
                    })
                except Exception as e:
                    logger.warning(f"å‹•ç”»ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
                    continue
            
            # ãƒˆãƒ¬ãƒ³ãƒ‰ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
            if trends:
                trends.sort(key=lambda x: x['trend_score'], reverse=True)
                for i, trend in enumerate(trends, 1):
                    trend['rank'] = i
            
            if trends:
                logger.info(f"æ€¥ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰è¨ˆç®—å®Œäº†: 1ä½={trends[0]['trend_score']:,.0f}ç‚¹, 25ä½={trends[-1]['trend_score']:,.0f}ç‚¹")
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            self.save_to_cache(region_code, 'rising', trends)
            logger.info(f"âœ… YouTubeæ€¥ä¸Šæ˜‡: å¤–éƒ¨APIã‹ã‚‰{len(trends)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ã—ã¾ã—ãŸ")
            
            return {
                'data': trends,
                'status': 'api_fetched',
                'region_code': region_code
            }
            
        except Exception as e:
            logger.error(f"YouTube Data APIã§ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return {'error': f'YouTube Data APIã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}'}

    def save_to_cache(self, region_code: str, trend_type: str, trends_data: list):
        """YouTube Trendsãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        try:
            self.db.save_youtube_trends_to_cache(trends_data, region_code, trend_type)
            # cache_statusãƒ†ãƒ¼ãƒ–ãƒ«ã‚‚æ›´æ–°
            self._update_cache_status('youtube_trends', len(trends_data))
        except Exception as e:
            logger.error(f"YouTubeã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            raise
    
    def _update_cache_status(self, cache_key, data_count):
        """cache_statusãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ›´æ–°"""
        try:
            from datetime import datetime
            now = datetime.now()
            
            with self.db.get_connection() as conn:
                with conn.cursor() as cursor:
                    cursor.execute("""
                        INSERT INTO cache_status (cache_key, last_updated, data_count)
                        VALUES (%s, %s, %s)
                        ON CONFLICT (cache_key) DO UPDATE SET
                            last_updated = EXCLUDED.last_updated,
                            data_count = EXCLUDED.data_count
                    """, (cache_key, now, data_count))
                    conn.commit()
        except Exception as e:
            logger.error(f"cache_statusæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)

    def get_from_cache(self, region_code: str, trend_type: str = 'trending'):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰YouTube Trendsãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        cached_data = self.db.get_youtube_trends_from_cache(region_code, trend_type)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ã«ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’è¿½åŠ ï¼ˆrankãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒãªã„å ´åˆï¼‰
        if cached_data:
            # æ€¥ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰ã®å ´åˆã¯ã€ãƒˆãƒ¬ãƒ³ãƒ‰ã‚¹ã‚³ã‚¢ã¾ãŸã¯è¦–è´å›æ•°ã§å†ã‚½ãƒ¼ãƒˆã—ã¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’è¨­å®š
            if trend_type == 'rising':
                # ãƒˆãƒ¬ãƒ³ãƒ‰ã‚¹ã‚³ã‚¢ãŒã‚ã‚‹å ´åˆã¯ãã‚Œã§ã‚½ãƒ¼ãƒˆã€ãªã„å ´åˆã¯è¦–è´å›æ•°ã§ã‚½ãƒ¼ãƒˆ
                if any(item.get('trend_score') is not None for item in cached_data):
                    # ãƒˆãƒ¬ãƒ³ãƒ‰ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆï¼ˆé™é †ï¼‰- Noneå€¤ã‚’0ã¨ã—ã¦æ‰±ã†
                    cached_data.sort(key=lambda x: x.get('trend_score') or 0, reverse=True)
                else:
                    # è¦–è´å›æ•°ã§ã‚½ãƒ¼ãƒˆï¼ˆé™é †ï¼‰- Noneå€¤ã‚’0ã¨ã—ã¦æ‰±ã†
                    cached_data.sort(key=lambda x: x.get('view_count') or 0, reverse=True)
                # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’è¨­å®š
                for i, item in enumerate(cached_data, 1):
                    item['rank'] = i
            else:
                # é€šå¸¸ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã®å ´åˆã¯é †ç•ªé€šã‚Šã«ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’è¨­å®š
                for i, item in enumerate(cached_data, 1):
                    if 'rank' not in item:
                        item['rank'] = i
        
        return cached_data

    def is_cache_valid(self, region_code: str, trend_type: str):
        """YouTubeã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯"""
        return self.db.is_youtube_cache_valid(region_code, trend_type)
    
    def _should_refresh_cache(self, region_code):
        """ä»Šæ—¥æ—¢ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–°ã—ãŸã‹ãƒã‚§ãƒƒã‚¯ï¼ˆæœ5æ™‚ã‹ã‚‰å¤œ12æ™‚ã¾ã§ï¼‰"""
        try:
            import pytz
            # æ—¥æœ¬æ™‚é–“ã§ç¾åœ¨æ™‚åˆ»ã‚’å–å¾—
            jst = pytz.timezone('Asia/Tokyo')
            now = datetime.now(jst)
            today = now.date()
            current_hour = now.hour
            
            # æ™‚é–“åˆ¶é™ï¼š5æ™‚ã‹ã‚‰24æ™‚ã¾ã§
            if not (5 <= current_hour < 24):
                logger.info(f"âš ï¸ æ™‚é–“å¤–ã§ã™ï¼ˆ{current_hour}æ™‚ï¼‰ã€‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                return False
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æœ€å¾Œã®æ›´æ–°æ—¥æ™‚ã‚’å–å¾—
            with self.db.get_connection() as conn:
                with conn.cursor() as cursor:
                    cursor.execute("""
                        SELECT last_updated 
                        FROM cache_status 
                        WHERE cache_key = %s
                    """, ('youtube_trends',))
                    
                    result = cursor.fetchone()
                    if result and result[0]:
                        last_refresh = result[0].date()
                        return last_refresh < today
                    return True  # åˆå›ã¯æ›´æ–°ã™ã‚‹
        except Exception as e:
            logger.error(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°æ—¥æ™‚ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return True
    
    def _update_refresh_time(self, region_code):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°æ—¥æ™‚ã‚’è¨˜éŒ²"""
        try:
            import pytz
            # æ—¥æœ¬æ™‚é–“ã§ç¾åœ¨æ™‚åˆ»ã‚’å–å¾—
            jst = pytz.timezone('Asia/Tokyo')
            now = datetime.now(jst)
            with self.db.get_connection() as conn:
                with conn.cursor() as cursor:
                    cursor.execute("""
                        INSERT INTO cache_status (platform, region, last_refresh_date, created_at)
                        VALUES (%s, %s, %s, %s)
                        ON CONFLICT (platform, region) 
                        DO UPDATE SET last_refresh_date = %s, updated_at = %s
                    """, ('youtube', region_code, now, now, 
                          now, now))
                    conn.commit()
        except Exception as e:
            logger.error(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°æ—¥æ™‚è¨˜éŒ²ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True) 